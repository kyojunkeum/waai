from __future__ import annotations

import asyncio
import calendar
import json
import logging
import os
import re
from urllib.parse import quote_plus
from datetime import datetime, date
from pathlib import Path
from typing import Any, List
from collections import deque
from typing import Tuple, Optional, Callable
import httpx
import yaml
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field, validator, ValidationError
from starlette.templating import Jinja2Templates

from mcp_client import (
    get_mood_stats,
    get_project_timeline,
    select_and_summarize,
)
from utils import ensure_dir, normalize_query, save_txt

# ë°ì´í„° ê²½ë¡œë¥¼ í™˜ê²½ë³€ìˆ˜ â†’ /data â†’ ë¡œì»¬ repo/data â†’ í™ˆ ê²½ë¡œ ìˆœì„œë¡œ í•´ì„
def _resolve_data_path(env_var: str, default_subpath: str, require_writable: bool = False) -> Path:
    candidates: list[Path] = []

    env_value = os.environ.get(env_var)
    if env_value:
        candidates.append(Path(env_value))

    candidates.append(Path("/data") / default_subpath)
    repo_root = Path(__file__).resolve().parent.parent
    candidates.append(repo_root / "data" / default_subpath)

    if require_writable:
        candidates.append(Path.home() / ".waai" / default_subpath)

    if require_writable:
        for path in candidates:
            try:
                path.mkdir(parents=True, exist_ok=True)
                return path
            except Exception:
                continue
        return candidates[-1]

    for path in candidates:
        if path.exists():
            return path
    return candidates[-1]

DIARY_ROOT = str(_resolve_data_path("DIARY_ROOT", "diary", require_writable=True))
DIARY_OUTPUT_DIR = Path(DIARY_ROOT)
DIARY_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

IDEAS_ROOT = str(_resolve_data_path("IDEAS_ROOT", "ideas"))
WEB_RESEARCH_ROOT = str(_resolve_data_path("WEB_RESEARCH_ROOT", "web_research", require_writable=True))
WEBRESEARCH_OUT_DIR = os.environ.get("WEBRESEARCH_OUT_DIR", "/memory/webresearch")
WORKS_ROOT = str(_resolve_data_path("WORKS_ROOT", "works"))
BIBLE_ROOT = str(_resolve_data_path("BIBLE_ROOT", "bible"))
CRITIQUE_OBJECTS_ROOT = _resolve_data_path("CRITIQUE_OBJECTS_ROOT", "critique/objects", require_writable=True)
CRITIQUE_RESULTS_ROOT = _resolve_data_path("CRITIQUE_RESULTS_ROOT", "critique/results", require_writable=True)
CRITIQUE_CRITERIA_PATH = _resolve_data_path("CRITIQUE_CRITERIA_PATH", "critique/criteria/í•©í‰ê¸°ì¤€ê·œì¹™.md")
CRITIQUE_CHUNK_MAX_CHARS = int(os.environ.get("CRITIQUE_CHUNK_MAX_CHARS", "6000"))
CRITIQUE_CHUNK_MAX_PARTS = int(os.environ.get("CRITIQUE_CHUNK_MAX_PARTS", "20"))

PLAYWRIGHT_SCHEDULE_PATH = Path(os.environ.get("PLAYWRIGHT_SCHEDULE_PATH", "/data/web_research/playwright_schedule.json"))
OUTPUT_ROOT = str(_resolve_data_path("OUTPUT_ROOT", "outputs", require_writable=True))
os.makedirs(OUTPUT_ROOT, exist_ok=True)
Path(WEB_RESEARCH_ROOT).mkdir(parents=True, exist_ok=True)
ensure_dir(WEBRESEARCH_OUT_DIR)
CRITIQUE_OBJECTS_ROOT.mkdir(parents=True, exist_ok=True)
CRITIQUE_RESULTS_ROOT.mkdir(parents=True, exist_ok=True)
PLAYWRIGHT_SCHEDULE_PATH.parent.mkdir(parents=True, exist_ok=True)
logger = logging.getLogger("waai-backend")

# ë°ì´í„° ë¡œë“œ ì•ˆì „ì¥ì¹˜
MAX_FILES_PER_TYPE = int(os.environ.get("MAX_FILES_PER_TYPE", "10"))

# ğŸ”¹ ê³µí†µ LLM ì„¤ì •
LLM_BACKEND = os.environ.get("LLM_BACKEND", "ollama").lower()

# Ollamaìš©
OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://ollama:11434")
MODEL_NAME = os.environ.get("MODEL_NAME", "qwen2:7b")

# OpenAI / í˜¸í™˜ ì„œë²„ìš© (ì„ íƒ)
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4.1-mini")

# docker-compose ê¸°ë³¸ í¬íŠ¸ëŠ” 7003. í•„ìš”ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ.
PLAYWRIGHT_MCP_URL = os.environ.get("PLAYWRIGHT_MCP_URL", "http://mcp-playwright:7003")
SEARXNG_URL = os.environ.get("SEARXNG_URL")

# YAML Front Matter ê²€ì¦ì‹œ ì‚¬ìš©
FRONT_MATTER_RE = re.compile(r"^\s*---\s*\n(.*?)\n---\s*\n?", re.S)

# ì¬ì‹œë„ ê²€ì¦ ì‹œ ì‚¬ìš©
LLM_VALIDATE_RETRIES=2
PLAN_MIN_CHARS=800
CRITIQUE_MIN_CHARS=600

class DiaryFormatRequest(BaseModel):
    date: str       # "2025-12-10"
    time: str       # "23:15"
    title: str
    raw_text: str


class DiaryFormatResponse(BaseModel):
    result: str     # ì™„ì„±ëœ Markdown (YAML í—¤ë” + ë³¸ë¬¸ + ì›ë³¸í…ìŠ¤íŠ¸)


class DiaryReformatRequest(BaseModel):
    markdown: str   # ê¸°ì¡´ md ì „ì²´ í…ìŠ¤íŠ¸


class DiaryReformatResponse(BaseModel):
    result: str     # ë³´ì •ëœ md ì „ì²´ í…ìŠ¤íŠ¸


class DataReformatRequest(BaseModel):
    doc_type: str   # idea | work | web_research | bible
    markdown: str   # ê¸°ì¡´ md ì „ì²´ í…ìŠ¤íŠ¸


class DataReformatResponse(BaseModel):
    result: str     # ë³´ì •ëœ md ì „ì²´ í…ìŠ¤íŠ¸

class PlanGenerateRequest(BaseModel):
    """
    Open WebUIì—ì„œ ì´ APIë¥¼ í˜¸ì¶œí•  ë•Œ ë„˜ê¸¸ ìˆ˜ ìˆëŠ” ì˜µì…˜ë“¤ì…ë‹ˆë‹¤.
    ì•„ë¬´ê²ƒë„ ì•ˆ ë„˜ê¸°ë©´ 'ì „ì²´ ì¼ê¸° ê¸°ë°˜ ê¸°íšì„œ'ë¥¼ ë§Œë“ ë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.
    """
    topic: str | None = None              # ê¸°íšì„œ ì œëª©/ì£¼ì œ (ì˜ˆ: "ìµœê·¼ ì¼ê¸° ê¸°ë°˜ ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ")
    keyword: str | None = None            # íŠ¹ì • í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë£¨ê³  ì‹¶ì„ ë•Œ
    start_date: str | None = None         # "2025-01-01" ì´ëŸ° ì‹
    end_date: str | None = None           # "2025-03-31"
    mode: str = "outline"                 # mcp-bridge summarize ëª¨ë“œ (outline/summary ë“±)
    output_format: str = "md"             # "txt" ë˜ëŠ” "md"
    extra_instruction: str | None = None  # "ê°€ì¡±/ì‹ ì•™ ë¹„ì¤‘ì„ ë” ê°•ì¡°í•´ì¤˜" ê°™ì€ ì¶”ê°€ ì§€ì‹œ


class PlanGeneratePromptOnlyRequest(BaseModel):
    prompt: str
    include: list[str] | None = None  # ì˜µì…˜: í¬í•¨í•  ë°ì´í„° íƒ€ì… ì§€ì •


class PlanGenerateResponse(BaseModel):
    title: str        # ê¸°íšì„œ ì œëª©
    content: str      # ê¸°íšì„œ ë³¸ë¬¸ (Open WebUIì—ì„œ ë°”ë¡œ ë³´ì—¬ì¤„ ë‚´ìš©)
    file_path: str    # /waai/data/outputs/... ì €ì¥ëœ ê²½ë¡œ
    sources: list[dict[str, Any]] = Field(default_factory=list)   # ê·¼ê±° ëª©ë¡ (í•„ìˆ˜, ë¹„ì–´ë„ í¬í•¨)


class PlanFromDataRequest(BaseModel):
    goal: str = "ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ ì œì‘"
    include: list[str] | None = None
    start_date: str | None = None
    end_date: str | None = None
    keyword: str | None = None
    extra_instruction: str | None = None
    prompt: str | None = None  # ìì—°ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë‚ ì§œ/í‚¤ì›Œë“œ/goal ì¶”ì¶œìš© (ì„ íƒ)


class PlanGenerateEnvelope(BaseModel):
    success: bool = True
    message: str = "ok"
    data: PlanGenerateResponse | None = None
    error: Any | None = None


class PlaywrightCrawlRequest(BaseModel):
    prompt: str | None = None
    keywords: list[str] = Field(default_factory=list, max_items=5)
    per_keyword: int = 2
    url: str | None = None
    timeout_ms: int | None = 20000


class PlaywrightCrawlResponse(BaseModel):
    saved_files: list[str] = Field(default_factory=list)
    count: int = 0
    keywords: list[str] = Field(default_factory=list)
    articles: list[dict[str, Any]] = Field(default_factory=list)


class WebSearchRequest(BaseModel):
    query: str
    max_results: int = 5
    engine: str = "google_news_rss"


class WebSearchFetchRequest(BaseModel):
    query: str
    max_results: int = 5
    engine: str = "google_news_rss"
    timeout_ms: int = 20000


class PlaywrightScheduleConfig(BaseModel):
    enabled: bool = False
    interval_minutes: int = 60
    keywords: list[str] = Field(default_factory=list, max_items=5)
    per_keyword: int = 2
    last_run: str | None = None
    last_error: str | None = None
    last_count: int | None = None


class CritiqueOptions(BaseModel):
    save_critique: bool = True
    save_work: bool | None = None
    chunked_critique: bool = False
    chunk_max_chars: int | None = None
    max_parts: int | None = None


class CritiqueRequest(BaseModel):
    title: str
    content: str
    options: CritiqueOptions | None = None
    extra_instruction: str | None = None


class CritiqueResponse(BaseModel):
    path: str
    critique: str

## ë°ì´í„° í¼ ìœ íš¨ì„± ê²€ì¦ ìŠ¤í‚¤ë§ˆ
class ReformatResult(BaseModel):
    front_matter: dict
    body: str
    tags: list[str]

app = FastAPI()
templates = Jinja2Templates(directory="templates")

# Static files (CSS ë“±)
app.mount("/static", StaticFiles(directory="static"), name="static")

# CORS for Open WebUI & dashboards
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000",
        "http://open-webui:8080",
        "http://open-webui:3000",
        "*",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def extract_front_matter(md: str) -> Tuple[dict, str, bool]:
    """
    Returns: (meta_dict, body_text, has_front_matter)
    """
    if not md:
        return {}, "", False
    m = FRONT_MATTER_RE.match(md)
    if not m:
        return {}, md, False

    raw_yaml = m.group(1)
    body = md[m.end():]
    try:
        meta = yaml.safe_load(raw_yaml) or {}
        if not isinstance(meta, dict):
            meta = {}
    except Exception:
        meta = {}
    return meta, body, True


def _iso_datetime_like(val: Any) -> bool:
    if isinstance(val, datetime):
        return True
    if isinstance(val, str):
        try:
            datetime.fromisoformat(val)
            return True
        except Exception:
            return False
    return False


def _is_list_of_str(val: Any, min_len: int = 0, max_len: int | None = None) -> bool:
    if not isinstance(val, list):
        return False
    if any(not isinstance(x, (str, int, float, bool)) for x in val):
        return False
    # ë¬¸ìì—´í™”ëŠ” í—ˆìš©í•˜ë˜, ê¸¸ì´ ì²´í¬
    if len(val) < min_len:
        return False
    if max_len is not None and len(val) > max_len:
        return False
    return True


def validate_diary_front_matter(meta: dict) -> None:
    """
    /api/diary/reformat-md ê²°ê³¼ë¬¼(ì¼ê¸° md) ê²€ì¦
    - front-matter ì¡´ì¬
    - date/time/type/mood/mood_score/tags/summary ë“± í•µì‹¬ í•„ë“œ ê²€ì¦
    """
    # required keys
    required = ["date", "time", "title", "type", "mood", "mood_score", "tags", "summary"]
    missing = [k for k in required if k not in meta]
    if missing:
        raise ValueError(f"missing keys: {missing}")

    # date format
    try:
        datetime.strptime(str(meta["date"]), "%Y-%m-%d")
    except Exception:
        raise ValueError("date must be YYYY-MM-DD")

    # time format "HH:MM"
    if not re.match(r"^\d{2}:\d{2}$", str(meta["time"])):
        raise ValueError("time must be HH:MM string")

    # type fixed
    if str(meta["type"]).strip().lower() != "diary":
        raise ValueError("type must be 'diary'")

    # mood_score range + 1 decimal
    try:
        ms = float(meta["mood_score"])
    except Exception:
        raise ValueError("mood_score must be a float")
    if ms < -1.0 or ms > 1.0:
        raise ValueError("mood_score must be between -1.0 and 1.0")
    if round(ms, 1) != ms:
        raise ValueError("mood_score must have at most 1 decimal place")

    # tags 3~7
    if not _is_list_of_str(meta.get("tags"), min_len=3, max_len=7):
        raise ValueError("tags must be list with 3~7 items")

    # summary length (ë„ˆë¬´ ì§§ìœ¼ë©´ í’ˆì§ˆìƒ ì‹¤íŒ¨ë¡œ ê°„ì£¼)
    if len(str(meta.get("summary") or "").strip()) < 10:
        raise ValueError("summary too short")


def validate_data_front_matter(meta: dict, doc_type: str) -> None:
    """
    /api/data/reformat-md ê²°ê³¼ë¬¼(idea/work/web_research/bible) ê²€ì¦
    """
    required = ["type", "title", "created_at", "updated_at", "tags", "topics", "people", "locations", "usage", "summary", "source"]
    missing = [k for k in required if k not in meta]
    if missing:
        raise ValueError(f"missing keys: {missing}")

    if str(meta["type"]).strip().lower() != doc_type:
        raise ValueError(f"type must be '{doc_type}'")

    if not _iso_datetime_like(meta.get("created_at")):
        raise ValueError("created_at must be ISO datetime")
    if not _iso_datetime_like(meta.get("updated_at")):
        raise ValueError("updated_at must be ISO datetime")

    if not _is_list_of_str(meta.get("tags"), min_len=1, max_len=20):
        raise ValueError("tags must be a list (min 1)")
    if not _is_list_of_str(meta.get("topics"), min_len=1, max_len=20):
        raise ValueError("topics must be a list (min 1)")
    if not _is_list_of_str(meta.get("people"), min_len=0, max_len=30):
        raise ValueError("people must be a list")
    if not _is_list_of_str(meta.get("locations"), min_len=0, max_len=30):
        raise ValueError("locations must be a list")
    if not _is_list_of_str(meta.get("usage"), min_len=0, max_len=10):
        raise ValueError("usage must be a list (min 0)")

    # source can be null/None/""/url/text
    src = meta.get("source")
    if src is not None and not isinstance(src, (str, dict, list)):
        raise ValueError("source must be string or null")


def validate_plan_front_matter(meta: dict) -> None:
    required = ["type", "title", "goal", "include", "created_at", "updated_at", "usage", "sources"]
    missing = [k for k in required if k not in meta]
    if missing:
        raise ValueError(f"missing keys: {missing}")

    if str(meta["type"]).strip().lower() != "plan":
        raise ValueError("type must be 'plan'")
    if not _iso_datetime_like(meta.get("created_at")):
        raise ValueError("created_at must be ISO datetime")
    if not _iso_datetime_like(meta.get("updated_at")):
        raise ValueError("updated_at must be ISO datetime")
    if not _is_list_of_str(meta.get("usage"), min_len=1):
        raise ValueError("usage must be a list (min 1)")

    inc = meta.get("include")
    if not isinstance(inc, list) or not inc:
        raise ValueError("include must be a non-empty list")


def validate_critique_front_matter(meta: dict) -> None:
    required = ["type", "object_title", "created_at", "updated_at", "source_object_file", "usage"]
    missing = [k for k in required if k not in meta]
    if missing:
        raise ValueError(f"missing keys: {missing}")

    if str(meta["type"]).strip().lower() != "critique":
        raise ValueError("type must be 'critique'")
    if not _iso_datetime_like(meta.get("created_at")):
        raise ValueError("created_at must be ISO datetime")
    if not _iso_datetime_like(meta.get("updated_at")):
        raise ValueError("updated_at must be ISO datetime")
    if not _is_list_of_str(meta.get("usage"), min_len=1):
        raise ValueError("usage must be a list (min 1)")


async def call_llm_with_front_matter_retry(
    prompt: str,
    validate_meta_fn: Callable[[dict], None],
    retries: int = 2,
    must_have_front_matter: bool = True,
) -> str:
    """
    LLMì´ Markdownì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •.
    - front-matter ì¡´ì¬ ì—¬ë¶€ + meta ê²€ì¦ì— ì‹¤íŒ¨í•˜ë©´ ì¬ì‹œë„
    - ì‹¤íŒ¨ ì‚¬ìœ ë¥¼ ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ì— í”¼ë“œë°±ìœ¼ë¡œ ë¶™ì„
    """
    last_md = ""
    last_err = ""

    for attempt in range(retries + 1):
        md = await call_llm(prompt)
        last_md = md

        meta, body, has_fm = extract_front_matter(md)

        try:
            if must_have_front_matter and not has_fm:
                raise ValueError("front-matter is missing. Output must start with YAML front-matter.")

            validate_meta_fn(meta)

            # í†µê³¼í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return md

        except Exception as exc:
            last_err = str(exc)
            if attempt >= retries:
                # ë§ˆì§€ë§‰ ì‹¤íŒ¨ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì§€ ë§ê³ , í˜¸ì¶œì ìª½ì—ì„œ ì—ëŸ¬ ì²˜ë¦¬í• ì§€ ì„ íƒ ê°€ëŠ¥
                raise ValueError(f"LLM output validation failed after retries: {last_err}")

            # ë‹¤ìŒ ì‹œë„ìš© í”„ë¡¬í”„íŠ¸ ê°•í™”
            prompt = (
                prompt
                + "\n\n"
                + "[ê²€ì¦ ì‹¤íŒ¨ í”¼ë“œë°±]\n"
                + f"- ì‹¤íŒ¨ ì‚¬ìœ : {last_err}\n"
                + "- ë°˜ë“œì‹œ YAML front-matterë¥¼ ìµœìƒë‹¨ì— 1ê°œë§Œ ë§Œë“¤ ê²ƒ.\n"
                + "- í‚¤ ëˆ„ë½/í˜•ì‹ ë¶ˆì¼ì¹˜(ë‚ ì§œ/ì‹œê°„/score/ë¦¬ìŠ¤íŠ¸)ë¥¼ ìˆ˜ì •í•´ ì¬ì¶œë ¥í•  ê²ƒ.\n"
                + "- ì„¤ëª… ê¸ˆì§€, ìµœì¢… Markdownë§Œ ì¶œë ¥.\n"
            )

    # ì—¬ê¸° ë„ë‹¬í•˜ì§€ ì•ŠìŒ
    return last_md

class DiarySchema(BaseModel):
    date: str                     # YYYY-MM-DD
    mood_score: float = Field(
        ge=-1.0,
        le=1.0,
        description="ê°ì • ì ìˆ˜ (-1.0 ~ 1.0, ì†Œìˆ˜ì  1ìë¦¬)"
    )
    summary: str
    body: str
    tags: List[str]

    @validator("date")
    def validate_date(cls, v):
        datetime.strptime(v, "%Y-%m-%d")
        return v

    @validator("mood_score")
    def validate_mood_score_precision(cls, v):
        if round(v, 1) != v:
            raise ValueError("mood_score must have at most 1 decimal place")
        return v


async def call_llm(prompt: str) -> str:
    """
    WAAI ë°±ì—”ë“œìš© ê³µí†µ LLM í˜¸ì¶œ í•¨ìˆ˜
    - LLM_BACKEND=ollama  : Ollama /api/generate
    - LLM_BACKEND=openai  : OpenAI ë˜ëŠ” í˜¸í™˜ ì„œë²„ /v1/chat/completions
    """
    backend = LLM_BACKEND

    # 1) Ollama
    if backend == "ollama":
        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(
                f"{OLLAMA_URL}/api/generate",
                json={
                    "model": MODEL_NAME,
                    "prompt": prompt,
                    "stream": False,
                },
            )
            resp.raise_for_status()
            data = resp.json()
            return data.get("response", "")

    # 2) OpenAI / í˜¸í™˜ ì„œë²„
    elif backend == "openai":
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": OPENAI_MODEL,
            "messages": [
                {"role": "user", "content": prompt},
            ],
        }
        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(
                f"{OPENAI_BASE_URL}/chat/completions",
                headers=headers,
                json=payload,
            )
            resp.raise_for_status()
            data = resp.json()
            return data["choices"][0]["message"]["content"]

    else:
        raise RuntimeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM_BACKEND: {backend}")


def standard_response(success: bool = True, message: str = "ok", data: Any = None, error: Any = None):
    """
    OpenWebUI HTTP Tool ì—°ë™ì„ ìœ„í•œ ê³µí†µ ì‘ë‹µ í¬ë§·.
    - success: bool
    - message: í•œ ì¤„ ì„¤ëª…
    - data: ì£¼ìš” ì‘ë‹µ í˜ì´ë¡œë“œ (dict/ëª¨ë¸)
    - error: ì—ëŸ¬ ìƒì„¸ (ì—†ìœ¼ë©´ None)
    """
    return {
        "success": success,
        "message": message,
        "data": data,
        "error": error,
    }


def save_plan_output(content: str) -> str:
    """
    V3.1 ìš”êµ¬ì‚¬í•­: /data/outputs/plan_YYYYMMDD.md í˜•ì‹ìœ¼ë¡œ ì €ì¥.
    - ë™ì¼ ë‚ ì§œì— ì—¬ëŸ¬ ë²ˆ ìƒì„± ì‹œì—ëŠ” ì¤‘ë³µì„ í”¼í•˜ê¸° ìœ„í•´ _HHMMSS ë¥¼ ë¶™ì¸ë‹¤.
    """
    today = datetime.now()
    date_part = today.strftime("%Y%m%d")
    base_name = f"plan_{date_part}.md"
    path = Path(OUTPUT_ROOT) / base_name
    if path.exists():
        suffix = today.strftime("%H%M%S")
        path = Path(OUTPUT_ROOT) / f"plan_{date_part}_{suffix}.md"
    path.write_text(content, encoding="utf-8")
    return str(path)


def extract_json_block(text: str) -> str:
    """
    LLMì´ ì•ë’¤ë¡œ ë©˜íŠ¸ë¥¼ ë¶™ì´ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ JSON ë¸”ë¡ë§Œ ì¶”ì¶œ.
    """
    match = re.search(r"\{.*\}", text, re.S)
    return match.group(0) if match else "{}"


def _format_date(year: int, month: int, day: int | None = None, month_end: bool = False) -> str:
    if day is None:
        day = calendar.monthrange(year, month)[1] if month_end else 1
    return f"{year:04d}-{month:02d}-{day:02d}"


def _extract_keyword_from_prompt(user_prompt: str) -> str | None:
    kw_match = re.search(r"[\"'â€œâ€â€˜â€™]([^\"'â€œâ€â€˜â€™]{1,20})[\"'â€œâ€â€˜â€™]", user_prompt)
    if kw_match:
        keyword = kw_match.group(1).strip()
        if keyword:
            return keyword
    return None


def _extract_keywords_from_prompt(prompt: str, limit: int = 5) -> list[str]:
    """
    ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ: ë”°ì˜´í‘œ/ì‰¼í‘œ/ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ ìµœëŒ€ limitê°œ.
    """
    if not prompt:
        return []
    # ë”°ì˜´í‘œë¡œ ê°ì‹¼ í‘œí˜„ ìš°ì„ 
    quoted = re.findall(r"[\"'â€œâ€â€˜â€™]([^\"'â€œâ€â€˜â€™]{1,30})[\"'â€œâ€â€˜â€™]", prompt)
    words: list[str] = []
    words.extend([w.strip() for w in quoted if w.strip()])
    # ì‰¼í‘œ/ê³µë°± ìŠ¤í”Œë¦¿
    parts = re.split(r"[,\s]+", prompt)
    for p in parts:
        p = p.strip()
        if not p:
            continue
        # ê¸¸ì´ 1 ê¸€ìëŠ” ë¬´ì‹œ
        if len(p) < 2:
            continue
        words.append(p)
    # ì¤‘ë³µ ì œê±°, ìµœëŒ€ limit
    deduped: list[str] = []
    for w in words:
        if w not in deduped:
            deduped.append(w)
        if len(deduped) >= limit:
            break
    return deduped


def rule_based_plan_parse(user_prompt: str) -> dict[str, str | None]:
    """
    ë¹ ë¥´ê²Œ íŒŒì‹±í•  ìˆ˜ ìˆëŠ” ìš”ì†Œ(ë‚ ì§œ/í‚¤ì›Œë“œ)ëŠ” ë£° ê¸°ë°˜ìœ¼ë¡œ ë¨¼ì € ì¶”ì¶œ.
    - ëª…í™•í•œ ë‚ ì§œ ë²”ìœ„ê°€ ë³´ì´ë©´ ë°”ë¡œ ì‚¬ìš©
    - ì• ë§¤í•œ í‘œí˜„ì€ LLM íŒŒì„œê°€ ë³´ì™„
    """
    keyword = _extract_keyword_from_prompt(user_prompt)

    # 1) "2025ë…„ 10ì›”ë¶€í„° 11ì›”ê¹Œì§€" ì²˜ëŸ¼ ì›” ë²”ìœ„
    month_range = re.search(
        r"(?P<start_year>\d{4})ë…„\s*(?P<start_month>\d{1,2})ì›”\s*(?:ë¶€í„°|~|-|â€“)?\s*(?:(?P<end_year>\d{4})ë…„\s*)?(?P<end_month>\d{1,2})ì›”",
        user_prompt,
    )
    if month_range:
        start_year = int(month_range.group("start_year"))
        start_month = int(month_range.group("start_month"))
        end_year = int(month_range.group("end_year") or start_year)
        end_month = int(month_range.group("end_month"))
        return {
            "start_date": _format_date(start_year, start_month),
            "end_date": _format_date(end_year, end_month, month_end=True),
            "keyword": keyword,
        }

    # 2) 2025-10-01 ~ 2025-11-30 ê°™ì€ ë‚ ì§œ ë²”ìœ„
    date_range = re.search(
        r"(?P<start_year>\d{4})[./-](?P<start_month>\d{1,2})[./-](?P<start_day>\d{1,2})\s*(?:ë¶€í„°|~|-|â€“|to)\s*(?:(?P<end_year>\d{4})[./-])?(?P<end_month>\d{1,2})[./-](?P<end_day>\d{1,2})",
        user_prompt,
    )
    if date_range:
        start_year = int(date_range.group("start_year"))
        start_month = int(date_range.group("start_month"))
        start_day = int(date_range.group("start_day"))
        end_year = int(date_range.group("end_year") or start_year)
        end_month = int(date_range.group("end_month"))
        end_day = int(date_range.group("end_day"))
        return {
            "start_date": _format_date(start_year, start_month, start_day),
            "end_date": _format_date(end_year, end_month, end_day),
            "keyword": keyword,
        }

    # 2-1) 12ì›” 10ì¼ë¶€í„° 12ì›” 18ì¼ê¹Œì§€ ê°™ì€ 'ì—°ë„ ì—†ëŠ”' ë‚ ì§œ ë²”ìœ„
    day_range = re.search(
        r"(?P<start_month>\d{1,2})ì›”\s*(?P<start_day>\d{1,2})ì¼?\s*(?:ë¶€í„°|~|-|â€“|to)\s*(?P<end_month>\d{1,2})ì›”\s*(?P<end_day>\d{1,2})ì¼?",
        user_prompt,
    )
    if not day_range:
        day_range = re.search(
            r"(?P<start_month>\d{1,2})[./-](?P<start_day>\d{1,2})\s*(?:ë¶€í„°|~|-|â€“|to)\s*(?P<end_month>\d{1,2})[./-](?P<end_day>\d{1,2})",
            user_prompt,
        )
    if day_range:
        year = datetime.now().year
        start_month = int(day_range.group("start_month"))
        start_day = int(day_range.group("start_day"))
        end_month = int(day_range.group("end_month"))
        end_day = int(day_range.group("end_day"))
        end_year = year + 1 if end_month < start_month else year
        return {
            "start_date": _format_date(year, start_month, start_day),
            "end_date": _format_date(end_year, end_month, end_day),
            "keyword": keyword,
        }

    # 3) ë‹¨ì¼ ë‚ ì§œë‚˜ ì›”ë§Œ ì§€ì •ëœ ê²½ìš° â†’ ì›” ì „ì²´ ë²”ìœ„ë¡œ ê°„ì£¼
    single_date = re.search(
        r"(?P<year>\d{4})ë…„\s*(?P<month>\d{1,2})ì›”(?:\s*(?P<day>\d{1,2})ì¼)?",
        user_prompt,
    )
    if not single_date:
        single_date = re.search(
            r"(?P<year>\d{4})[./-](?P<month>\d{1,2})(?:[./-](?P<day>\d{1,2}))?",
            user_prompt,
        )

    if single_date:
        year = int(single_date.group("year"))
        month = int(single_date.group("month"))
        day_str = single_date.group("day")
        if day_str:
            day = int(day_str)
            start_date = _format_date(year, month, day)
            end_date = _format_date(year, month, day)
        else:
            start_date = _format_date(year, month)
            end_date = _format_date(year, month, month_end=True)
        return {
            "start_date": start_date,
            "end_date": end_date,
            "keyword": keyword,
        }

    # 3-1) ì—°ë„ ì—†ì´ ì›”/ì¼ë§Œ ì§€ì •ëœ ê²½ìš°ëŠ” í˜„ì¬ ì—°ë„ë¡œ ë³´ì •
    month_day = re.search(
        r"(?P<month>\d{1,2})ì›”\s*(?P<day>\d{1,2})ì¼?",
        user_prompt,
    )
    if not month_day:
        month_day = re.search(
            r"(?P<month>\d{1,2})[./-](?P<day>\d{1,2})",
            user_prompt,
        )
    if month_day:
        year = datetime.now().year
        month = int(month_day.group("month"))
        day = int(month_day.group("day"))
        return {
            "start_date": _format_date(year, month, day),
            "end_date": _format_date(year, month, day),
            "keyword": keyword,
        }

    return {
        "start_date": None,
        "end_date": None,
        "keyword": keyword,
    }


def save_plan_parse_log(user_prompt: str, raw_response: str, parsed: PlanGenerateRequest, rule_hints: dict[str, str | None]):
    """
    íŒŒì„œ ê²°ê³¼ë¥¼ /data/outputs ìª½ì— ë‚¨ê²¨ ìš´ì˜ ì‹œ ì¶”ì  ê°€ëŠ¥í•˜ê²Œ.
    """
    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = Path(OUTPUT_ROOT) / f"plan_parse_log_{ts}.json"
        payload = {
            "user_prompt": user_prompt,
            "rule_hints": rule_hints,
            "llm_raw": raw_response,
            "parsed": parsed.dict(),
        }
        path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        return str(path)
    except Exception:
        return None


async def parse_plan_request_with_llm(user_prompt: str) -> tuple[PlanGenerateRequest, str]:
    parser_prompt = f"""
ë„ˆëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ ë¬¸ì¥ì„ PlanGenerateRequest JSONìœ¼ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì„œë‹¤.
ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆì˜ í‚¤ë§Œ ì‚¬ìš©í•´ì„œ JSONë§Œ ì¶œë ¥í•´ë¼(ì„¤ëª… ê¸ˆì§€).

ìŠ¤í‚¤ë§ˆ í‚¤:
topic, keyword, start_date, end_date, mode, output_format, extra_instruction

ê·œì¹™:
- ë‚ ì§œëŠ” YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ.
- ë‚ ì§œê°€ ì—†ìœ¼ë©´ start_date/end_dateëŠ” null.
- topicì€ ê¸°íšì„œ ì œëª©/ì£¼ì œë¡œ ê°€ì¥ ì ì ˆí•œ ì§§ì€ ë¬¸ì¥.
- keywordëŠ” ëŒ€í‘œ í‚¤ì›Œë“œ 1ê°œ(ì—†ìœ¼ë©´ null).
- extra_instructionì—ëŠ” ê°•ì¡°ì /í†¤/ì œì™¸ìš”ì†Œ ë“± ì¶”ê°€ ì§€ì‹œë¥¼ ë„£ì–´ë¼.
- modeëŠ” outline/summary ì¤‘ í•˜ë‚˜(ì—†ìœ¼ë©´ outline).
- output_formatì€ md/txt ì¤‘ í•˜ë‚˜(ì—†ìœ¼ë©´ md).

ì‚¬ìš©ì ìš”ì²­:
\"\"\"{user_prompt}\"\"\"
""".strip()

    raw = await call_llm(parser_prompt)
    js = extract_json_block(raw)

    try:
        data = json.loads(js)
    except json.JSONDecodeError:
        data = {}

    data.setdefault("mode", "outline")
    data.setdefault("output_format", "md")

    try:
        return PlanGenerateRequest(**data), raw
    except ValidationError:
        return PlanGenerateRequest(
            topic="ì¼ê¸° ê¸°ë°˜ ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ",
            extra_instruction=user_prompt,
        ), raw


async def parse_plan_request(user_prompt: str) -> PlanGenerateRequest:
    """
    1) ë£° ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•œ ë‚ ì§œ/í‚¤ì›Œë“œ ë¨¼ì € ì¡ê¸°
    2) ë‚˜ë¨¸ì§€ëŠ” LLM íŒŒì„œì—ê²Œ JSON ìŠ¤í‚¤ë§ˆë¡œ ê°•ì œ
    3) ë¡œê·¸ë¥¼ ë‚¨ê²¨ ìš´ì˜ ì¤‘ íŒŒì„œ í’ˆì§ˆ ì¶”ì 
    """
    rule_hints = rule_based_plan_parse(user_prompt)
    raw_response = ""

    try:
        parsed, raw_response = await parse_plan_request_with_llm(user_prompt)
    except Exception:
        parsed = PlanGenerateRequest(
            topic="ì¼ê¸° ê¸°ë°˜ ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ",
            extra_instruction=user_prompt,
        )

    # ë£° ê¸°ë°˜ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš© (LLMì´ ì• ë§¤í•˜ê²Œ ì¡ëŠ” ê²½ìš° ë®ì–´ì“°ê¸°)
    updates = {k: v for k, v in rule_hints.items() if v}
    if updates:
        parsed = parsed.copy(update=updates)

    save_plan_parse_log(user_prompt, raw_response, parsed, rule_hints)
    return parsed


# =========================
# âœ… NEW: ì¼ê¸° í¬ë§·íŒ… API
# =========================

def build_diary_format_prompt(req: DiaryFormatRequest) -> str:
    """
    Qwen ê³„ì—´ ëª¨ë¸ì— ìµœì í™”ëœ ê³ ë„í™” í”„ë¡¬í”„íŠ¸.
    - ê°ì • ë¶„ì„
    - tags ìë™ ì¶”ì¶œ (3~7ê°œ)
    - people ì¶”ì¶œ
    - location ì¶”ì •
    - projects ìë™ ë¶„ë¥˜ (ì†Œì„¤ì•„ì´ë””ì–´/NGO/IT/ê°€ì¡± ì¤‘ ìµœëŒ€ 2ê°œ)
    - scene_potential ìë™ ì—¬ë¶€
    - summary 1ë¬¸ì¥ ìƒì„±
    - Markdown + YAML ì™„ì„±
    """

    return f"""
ë‹¹ì‹ ì€ \"ê°œì¸ ì¼ê¸° ë¶„ì„ & êµ¬ì¡°í™” ì „ë¬¸ê°€\"ì´ì \"Qwen ìµœì í™” LLM\"ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ ì‚¬ìš©ìì˜ ì›ë³¸ ì¼ê¸°ë¥¼ ì½ê³  ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ êµ¬ì¡°í™”ëœ ì¼ê¸° Markdown íŒŒì¼ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

========================================================
ğŸ¯ ì¶œë ¥ ê·œì¹™(ì•„ì£¼ ì¤‘ìš”): ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤
========================================================

1) ë°˜ë“œì‹œ YAML í”„ë¡ íŠ¸ë§¤í„°ë¶€í„° ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. (`---` ë¡œ ì‹œì‘í•˜ê³  `---` ë¡œ ë‹«ìŒ)

2) YAML í•„ë“œ ê·œì¹™:
   - date: {req.date} (ë³€ê²½ ê¸ˆì§€)
   - time: \"{req.time}\" (ë³€ê²½ ê¸ˆì§€)
   - title: ì›ë³¸ ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì •ë¦¬í•˜ë˜ ê³¼í•œ ì°½ì‘ì€ ê¸ˆì§€
   - mood: ì˜ì–´ ì†Œë¬¸ì ìŠ¤ë„¤ì´í¬ì¼€ì´ìŠ¤ (ì˜ˆ: happy, sad, anxious_relief, exhausted, mixed_hopeful)
   - mood_score: -1.0 ~ +1.0 ì‹¤ìˆ˜ (ë§¤ìš° ìš°ìš¸ -1.0 / í‰ë²” 0.0 / ë§¤ìš° ê¸ì •ì  +1.0)
   - tags: ì›ë³¸ ì¼ê¸°ì˜ í•µì‹¬ í‚¤ì›Œë“œ 3~7ê°œë¥¼ í•œêµ­ì–´ ë°°ì—´ë¡œ
   - people: ë“±ì¥í•œ ì¸ë¬¼ ë˜ëŠ” ê´€ê³„(ì•„ë‚´, ë”¸, ë¶€ëª¨ë‹˜ ë“±)
   - location: \"home\", \"office\", \"cafe\", \"outdoor\" ë“± í•œ ë‹¨ì–´ë¡œ ìš”ì•½
   - type: \"diary\" ê³ ì •
   - projects: \"ì†Œì„¤ì•„ì´ë””ì–´\", \"NGO\", \"IT\", \"ê°€ì¡±\" ì¤‘ ì¼ê¸°ì™€ ê°€ì¥ ê´€ë ¨ ë†’ì€ 1~2ê°œ ì„ íƒ
   - scene_potential: ì›ë³¸ ì¼ê¸°ê°€ ì†Œì„¤ ì¥ë©´ìœ¼ë¡œ í™•ì¥í•  ê°€ì¹˜ê°€ ìˆìœ¼ë©´ true ì•„ë‹ˆë©´ false
   - summary: ì¼ê¸°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •êµí•˜ê²Œ ìš”ì•½

3) YAML ì•„ë˜ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ Markdown ì„¹ì…˜ì„ í¬í•¨í•©ë‹ˆë‹¤:
   # ì˜¤ëŠ˜ ìš”ì•½ (3ì¤„)
   # ì˜¤ëŠ˜ì˜ ì‚¬ê±´
   # ê°ì • / ìƒê°
   # ë°°ìš´ ê²ƒ / í†µì°°
   # ì†Œì„¤ ì•„ì´ë””ì–´ ë©”ëª¨ (ì˜µì…˜)
   # TODO / ë‹¤ìŒì— ì´ì–´ì„œ ì“¸ ê²ƒ (ì˜µì…˜)

4) ë§ˆì§€ë§‰ì— ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì¡´í•´ì•¼ í•©ë‹ˆë‹¤:
```text
{req.raw_text}
```
""".strip()


def build_plan_prompt(topic: str, diary_summary: str, extra_instruction: str | None = None) -> str:
    """
    qwen ê³„ì—´ / ì¼ë°˜ LLM ëª¨ë‘ ì˜ ë¨¹ê²Œ ì„¤ê³„í•œ ê¸°íšì„œ í”„ë¡¬í”„íŠ¸.
    diary_summaryì—ëŠ” mcp-bridgeê°€ ë§Œë“  ë‹¤ì¤‘ ì†ŒìŠ¤(diary/ideas/web_research/works/bible ë“±) ìš”ì•½/í†µê³„ ê²°ê³¼ë¥¼ ë„£ìŠµë‹ˆë‹¤.
    """
    extra = extra_instruction.strip() if extra_instruction else ""

    return f"""
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¼ê¸°ì™€ ì°½ì‘ ë…¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
'ë‹¨í¸ì†Œì„¤ ê¸°íš ì „ë¬¸ ì—ë””í„° & ìŠ¤í† ë¦¬ ì»¨ì„¤í„´íŠ¸'ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì‚¬ìš©ìê°€ ì‘ì„±í•œ ë‹¤ì¤‘ ì†ŒìŠ¤(md) ë°ì´í„°ì™€ ìš”ì•½ì…ë‹ˆë‹¤.
ì¼ê¸°ë¿ ì•„ë‹ˆë¼ ì•„ì´ë””ì–´(ideas), ì›¹ ë¦¬ì„œì¹˜(web_research), ê¸°ì¡´ ì‘í’ˆ(works), ì„±ê²½ ë©”ëª¨(bible)ê¹Œì§€ í™œìš©í•´ **ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ**ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

[ìš”ì²­ ë‹¨í¸ì†Œì„¤ ê¸°íš ì£¼ì œ]
- {topic}

[ë‹¤ì¤‘ ì†ŒìŠ¤ ì°½ì‘ ë°ì´í„° ìš”ì•½]
{diary_summary}

--------------------------------------
[ì‘ì„± ê·œì¹™ â€” ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ]
--------------------------------------

ì „ì²´ ê¸°íšì„œëŠ” ì•„ë˜ êµ¬ì¡°ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
í•œêµ­ì–´ë¡œ, ë¬¸í•™ì ì´ì§€ë§Œ ê³¼í•˜ì§€ ì•Šì€ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.
íŒ¨í„´Â·ëª¨í‹°í”„Â·ì •ì„œì˜ íë¦„ì„ ë¶„ì„í•˜ì—¬ â€˜ì†Œì„¤ì ì¸ ì˜ë¯¸â€™ë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.

# 1. ì†Œì„¤ ê°œìš”(Concept Overview)
- ì´ ì¼ê¸° ë°ì´í„°ì—ì„œ ë„ì¶œëœ í•µì‹¬ í…Œë§ˆ í•œ ë¬¸ë‹¨ ìš”ì•½
- ì´ì•¼ê¸°ì˜ ì£¼ì œ(Theme) ì œì•ˆ 1~2ê°œ
- ì†Œì„¤ì˜ ê°ì •ì  ìƒ‰ì±„(í†¤ & ë¬´ë“œ)

# 2. í•µì‹¬ ë©”ì‹œì§€ / ì£¼ì œ ì˜ì‹ (Theme)
- ì‚¬ìš©ìì˜ ì‚¶ì—ì„œ ë°˜ë³µë˜ëŠ” í•µì‹¬ ì •ì„œÂ·í†µì°°ì„ ë¬¸í•™ì  ì£¼ì œë¡œ ì •ë¦¬
- ì´ì•¼ê¸°ì˜ ì¤‘ì‹¬ ì§ˆë¬¸(Central Question) ì œì•ˆ
- ë©”ì‹œì§€ê°€ ë…ìì—ê²Œ ì „ë‹¬í•  ê°ì •ì  íš¨ê³¼

# 3. ë“±ì¥ì¸ë¬¼ ì„¤ê³„(Character Design)
- ì£¼ì¸ê³µ(Protagonist): ì„±ê²©Â·ê²°í•Â·ê°ˆë§Â·í•µì‹¬ ìƒì²˜
- ì£¼ìš” ì¸ë¬¼: ì•„ë‚´/ê°€ì¡±/ë˜ëŠ” ìƒì§•ì  ì¸ë¬¼ ë“± ì¼ê¸° ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„
- ì¸ë¬¼ ê°„ ê´€ê³„ì˜ ê¸´ì¥ êµ¬ì¡°
- ê°ì •ì  ë³€í™” ì•„í¬(Character Arc)

# 4. ì„¸ê³„ê´€ ë° ë°°ê²½(World & Setting)
- ì¼ê¸° ì† í˜„ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ í•œ â€˜ì‚¬ì‹¤ì  ì„¸ê³„â€™
- ê¸°ìˆ , ê°ì • ê¸°ìˆ (E.V.E ê°™ì€ ìš”ì†Œ), ì‚¬íšŒ ë¬¸ì œ(NGO/ë¹ˆê³¤ ë“±) ë“±
  - í˜„ì‹¤Â·ê·¼ë¯¸ë˜Â·ì´ˆí˜„ì‹¤ ì¤‘ ì–´ë–¤ í†¤ì´ ì–´ìš¸ë¦¬ëŠ”ì§€ ì œì•ˆ
- ë°°ê²½ì´ ìƒì§•í•˜ëŠ” ì˜ë¯¸

# 5. í”Œë¡¯ ì„¤ê³„(Plot Structure)
## 5-1. ì‚¬ê±´ì˜ íë¦„(Story Beats)
- ë°œë‹¨ â†’ ì „ê°œ â†’ ì „í™˜ â†’ ì ˆì • â†’ ê²°ë§ì˜ ìŠ¤í† ë¦¬ ë¼ì¸ ì œì•ˆ
- ì£¼ì¸ê³µì˜ ê°ì • ë³€í™”ê°€ ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ ë¬˜ì‚¬

## 5-2. ê°ˆë“±(Conflict)
- ì™¸ì  ê°ˆë“±(ê°€ì¡±, ì‚¬íšŒì  ì••ë°•, ê¸°ìˆ , ì¸ê°„ê´€ê³„ ë“±)
- ë‚´ì  ê°ˆë“±(ë‘ë ¤ì›€, ìƒì²˜, ì‹ ì•™, íšŒë³µ, ìê¸° ì˜ì‹¬ ë“±)
- ê°ˆë“±ì´ ì£¼ì œì™€ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ ì„¤ëª…

## 5-3. ì¥ë©´ ì•„ì´ë””ì–´(Scene Ideas)
- ê¸°ì–µÂ·ì¼ê¸°ì—ì„œ ì§ì ‘ ì¶”ì¶œí•œ â€˜ì¥ë©´ì„± ìˆëŠ” ìˆœê°„â€™ 3~7ê°œ
- ì´ ì¥ë©´ë“¤ì„ ìŠ¤í† ë¦¬ì— ë°°ì¹˜í•˜ëŠ” ë°©ì‹ ì œì•ˆ

# 6. ìƒì§• / ëª¨í‹°í”„(Motifs & Symbols)
- ë°˜ë³µì ìœ¼ë¡œ ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œÂ·ê°ì •Â·ì‚¬ê±´ì„ ë¬¸í•™ì  ëª¨í‹°í”„í™”
- ì˜ˆ: ì¹¨ë¬µ, íšŒë³µ, ê³ í†µ, ê°€ì¡±, ê¸°ìˆ  vs ì¸ê°„, ì˜¤í•´, ì‚¬ë‘ ë“±
- ì†Œì„¤ ì† ìƒì§•ì  ì¥ì¹˜ë¡œì˜ ë³€í™˜ ì œì•ˆ

# 7. ì‘í’ˆ í†¤ & ë¬¸ì²´ ì œì•ˆ(Style Recommendation)
- ì•„ë˜ëŠ” 'ì„ íƒì‚¬í•­'ì´ë©°, ë°˜ë“œì‹œ ì¼ê¸° ê¸°ë°˜ ê·¼ê±°ë¥¼ ë¨¼ì € ì œì‹œí•œ ë’¤ì—ë§Œ ì œì•ˆí•˜ë¼.
- ì¶”ì²œ ì‘ê°€/ë¬¸ì²´ëŠ” 1~2ê°œë§Œ ì œì‹œí•˜ë¼.
- ì´ ë‹¨í¸ì— ì–´ìš¸ë¦¬ëŠ” ë¬¸ì¥ ìŠ¤íƒ€ì¼
- ëŠë¦°/ë¹ ë¥¸/ì„œì •ì /ì••ì¶•ì  ë“± ë¬¸ì²´ ê°€ì´ë“œ

# 8. ë…ì ê²½í—˜ ì„¤ê³„(Reader Experience)
- ë…ìê°€ ëŠë‚„ ê°ì • ì—¬ì •
- ì†Œì„¤ì´ ë‚¨ê¸¸ â€˜ë’·ë§›â€™ í˜¹ì€ ì—¬ìš´

# 9. í–¥í›„ ë°œì „ ê°€ëŠ¥ì„±
- ì¥í¸ í™•ì¥ ê°€ëŠ¥ì„± ì—¬ë¶€
- ë™ì¼ ì„¸ê³„ê´€ì—ì„œì˜ ì¶”ê°€ ë‹¨í¸ ì•„ì´ë””ì–´
- ì£¼ì¸ê³µ ë˜ëŠ” ì„¤ì •ì„ í™•ì¥í•  ë°©ì•ˆ

# 10. ìµœì¢… ìš”ì•½(One-Paragraph Logline)
- ìœ„ ê¸°íšì„œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•œ ë¡œê¸€ë¼ì¸(logline)

--------------------------------------
[í†¤ & ìŠ¤íƒ€ì¼]
--------------------------------------
- ë¬¸í•™ì ì´ë˜ ë‚œí•´í•˜ì§€ ì•Šê²Œ.
- ì‚¬ìš©ìì˜ ì‚¶ì„ â€œì†Œì„¤ì  ì¬ë£Œâ€ë¡œ ì¡´ì¤‘í•˜ë©° í•´ì„.
- ì¼ê¸° ì† ìƒì²˜Â·ë¯¿ìŒÂ·ê°ì •ì€ ì‹ ì¤‘í•˜ê²Œ ë‹¤ë£¨ê³ ,
  í¬ë§ì˜ ë°©í–¥ì„±ë„ ìƒì§€ ì•Šë„ë¡.
- ìŠ¤í† ë¦¬ëŠ” ì‹¤í˜„ ê°€ëŠ¥í•œ êµ¬ì²´ì  í˜•íƒœë¡œ ì œì•ˆ.
--------------------------------------
--------------------------------------
[ê·¼ê±° ê¸°ë°˜ ì‘ì„± â€” ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ]
--------------------------------------
- ì´ ê¸°íšì„œëŠ” 'ì‚¬ìš©ìì˜ ì‹¤ì œ ê¸°ë¡'ì— ê·¼ê±°í•´ì•¼ í•œë‹¤.
- diary/ideas/web_research/works/bible ëª¨ë“  íƒ€ì… ë°ì´í„°ë¥¼ í™œìš©í•œë‹¤.
- ê° ì„¹ì…˜ë§ˆë‹¤ ì•„ë˜ í˜•ì‹ì˜ 'ê·¼ê±°'ë¥¼ ìµœì†Œ 2ê°œ ì´ìƒ í¬í•¨í•˜ë¼.
- ê·¼ê±°ëŠ” ë°˜ë“œì‹œ ì›ë¬¸ ë˜ëŠ” íŒŒì¼ëª…ì„ ì¸ìš©í•´ë¼. ì˜ˆ)
  - [source: diary/2025-12-01_x.md] "<ë¬¸êµ¬>"
  - [source: ideas/â€¦], [source: web_research/â€¦], [source: works/â€¦], [source: bible/â€¦]
- ì„±ê²½(BIBLE) ì¸ìš© ì‹œ [ì„±ê²½: ì±… ì¥:ì ˆ (ë²ˆì—­)] í˜•ì‹ì„ ì‚¬ìš©í•˜ê³ , ë°©í–¥/í†µì°°ì„ ë•ëŠ” ì°¸ê³ ë¡œë§Œ í™œìš©í•œë‹¤.
- ì¸ìš©ì€ ê³¼ì¥í•˜ì§€ ë§ê³ , ìš”ì•½ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ë¼.

[ê·¼ê±° í‘œê¸° í˜•ì‹]
- ê·¼ê±°: (YYYY-MM-DD) "<ìš”ì•½ì—ì„œ ë‚˜ì˜¨ í•µì‹¬ ë¬¸ì¥/í‚¤ì›Œë“œ>" â†’ ì™œ ì´ ê·¼ê±°ê°€ ì„¹ì…˜ì„ ë’·ë°›ì¹¨í•˜ëŠ”ì§€ 1ë¬¸ì¥ ì„¤ëª…

[ê¸ˆì§€]
- ê·¼ê±° ì—†ì´ ì¼ë°˜ë¡ ìœ¼ë¡œë§Œ ì“°ëŠ” ë¬¸ì¥(ì˜ˆ: 'ëˆ„êµ¬ë‚˜ ì„±ì¥í•œë‹¤', 'ê°ë™ì„ ì¤€ë‹¤')ì€ ê¸ˆì§€í•œë‹¤.
- ê·¼ê±°ê°€ ë¹ˆì•½í•˜ë©´ 'ê·¼ê±°ê°€ ë¶€ì¡±í•¨'ì„ ëª…ì‹œí•˜ê³ , ì–´ë–¤ ì •ë³´ê°€ ë” í•„ìš”í•˜ë‹¤ê³  ì œì•ˆí•˜ë¼.

ì¶”ê°€ ì°¸ê³  ì§€ì‹œì‚¬í•­(ìˆìœ¼ë©´ ë°˜ì˜, ì—†ìœ¼ë©´ ë¬´ì‹œ ê°€ëŠ¥):
{extra}
""".strip()


def slugify_filename(text: str) -> str:
    text = text.strip().replace(" ", "")
    for ch in "/\\?%*:|\"<>":
        text = text.replace(ch, "-")
    return text or "note"


async def _call_playwright_crawl(payload: dict[str, Any]) -> dict[str, Any] | None:
    try:
        async with httpx.AsyncClient(timeout=180.0) as client:
            resp = await client.post(f"{PLAYWRIGHT_MCP_URL}/crawl", json=payload)
            resp.raise_for_status()
            return resp.json()
    except Exception:
        return None


async def _call_playwright_fetch(url: str, timeout_ms: int | None = None) -> tuple[dict[str, Any] | None, str | None]:
    """
    Call Playwright MCP fetch endpoint and return (data, error_message).
    """
    payload: dict[str, Any] = {"url": url}
    if timeout_ms is not None:
        payload["timeout_ms"] = timeout_ms

    base_timeout = max(5.0, (timeout_ms or 20000) / 1000 + 5)
    try:
        async with httpx.AsyncClient(timeout=base_timeout) as client:
            resp = await client.post(f"{PLAYWRIGHT_MCP_URL.rstrip('/')}/fetch", json=payload)
    except Exception as exc:
        return None, f"http error: {exc}"

    if resp.status_code >= 400:
        return None, f"playwright mcp returned status {resp.status_code}"

    try:
        data = resp.json()
    except Exception as exc:
        return None, f"invalid json from playwright mcp: {exc}"

    return data, None


def _extract_article_payload(raw: Any) -> tuple[str | None, str | None, str | None]:
    """
    Extract (link, title, body) from various possible MCP payload shapes.
    """
    if not isinstance(raw, dict):
        return None, None, None

    payload = raw
    if isinstance(payload.get("data"), dict):
        payload = payload["data"]

    # try direct keys
    link = payload.get("link") or payload.get("url")
    title = payload.get("title") or payload.get("pageTitle") or payload.get("name")
    body = payload.get("body") or payload.get("text") or payload.get("content")

    # fallback: nested article/result objects
    if body is None:
        for key in ("article", "result", "item"):
            nested = payload.get(key)
            if isinstance(nested, dict):
                link = link or nested.get("link") or nested.get("url")
                title = title or nested.get("title") or nested.get("pageTitle")
                body = nested.get("body") or nested.get("text") or nested.get("content")
                if body:
                    break

    if isinstance(body, (dict, list)):
        body = json.dumps(body, ensure_ascii=False)
    if body is not None:
        body = str(body).strip()

    return link, title, body


ARTICLE_NOISE_KEYWORDS = [
    "ê³µìœ ",
    "ìŠ¤í¬ë©",
    "ì¸ì‡„",
    "ê¸€ì í¬ê¸°",
    "í°íŠ¸",
    "ëŒ“ê¸€",
    "êµ¬ë…",
    "ë¡œê·¸ì¸",
    "ì•±ì—ì„œ ë³´ê¸°",
    "ë°”ë¡œê°€ê¸°",
    "ê¸°ì",
    "í›„ì›",
    "ê´‘ê³ ",
    "ë¬´ë‹¨ì „ì¬",
    "ì¬ë°°í¬",
    "ì €ì‘ê¶Œ",
    "ë‰´ìŠ¤ ì œê³µ",
    "ê¸°ì‚¬ ì›ë¬¸",
]

ARTICLE_NOISE_PATTERNS = [
    r"ë¬´ë‹¨ì „ì¬\s*/\s*ì¬ë°°í¬ ê¸ˆì§€",
    r"copyright",
    r"â“’",
    r"ì‚¬ì§„\s*=\s*",
    r"ì˜ìƒ\s*=\s*",
    r"ê´€ë ¨\s*ê¸°ì‚¬",
    r"ê¸°ì‚¬\s*ì…ë ¥",
    r"ê¸°ì‚¬\s*ìŠ¹ì¸",
]


def _normalize_body_text(text: str) -> str:
    normalized = re.sub(r"\r\n?", "\n", text or "")
    normalized = re.sub(r"[ \t]+", " ", normalized)
    normalized = re.sub(r"\n{3,}", "\n\n", normalized)
    return normalized.strip()


def _split_paragraphs(text: str, limit: int = 32, max_chars: int = 6000) -> list[str]:
    paragraphs: list[str] = []
    total = 0
    for block in re.split(r"\n{2,}", text):
        block = block.strip()
        if not block:
            continue
        cleaned = re.sub(r"\s+", " ", block)
        if len(cleaned) < 8:
            continue
        total += len(cleaned)
        if total > max_chars:
            break
        paragraphs.append(cleaned)
        if len(paragraphs) >= limit:
            break
    return paragraphs


def _rule_based_article_cleanup(body: str | None) -> str:
    normalized = _normalize_body_text(body or "")
    if not normalized:
        return ""

    blocks = _split_paragraphs(normalized)
    cleaned: list[str] = []

    for block in blocks:
        lower = block.lower()
        if any(key in lower for key in ARTICLE_NOISE_KEYWORDS):
            continue
        if any(re.search(pat, block, re.I) for pat in ARTICLE_NOISE_PATTERNS):
            continue
        cleaned.append(block)

    # ì§§ì€ ë³¸ë¬¸ì´ë¼ë©´ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if not cleaned and normalized:
        return normalized

    # ì¤‘ë³µ ë‹¨ë½ ì œê±°
    seen: set[str] = set()
    deduped: list[str] = []
    for block in cleaned:
        if block not in seen:
            deduped.append(block)
            seen.add(block)

    return "\n\n".join(deduped)


def _build_llm_body_prompt(title: str, url: str | None, blocks: list[str]) -> str:
    blocks_json = json.dumps(blocks, ensure_ascii=False, indent=2)
    return f"""
ë„ˆëŠ” ë‰´ìŠ¤/ë¸”ë¡œê·¸ ê¸°ì‚¬ ë³¸ë¬¸ ì‹ë³„ ë° ì •ì œê¸°ë‹¤.
ì£¼ì–´ì§„ í›„ë³´ ë¬¸ë‹¨ ì¤‘ ê¸°ì‚¬ ë³¸ë¬¸ë§Œ ë‚¨ê¸°ê³  UI/ê´‘ê³ /ê³µìœ /í°íŠ¸ ì•ˆë‚´/ëŒ“ê¸€/êµ¬ë…/ì €ì‘ê¶Œ ë¬¸êµ¬ë¥¼ ì œê±°í•œë‹¤.

[ì¶œë ¥ë§Œ JSONìœ¼ë¡œ]
{{
  "is_article": true|false,
  "clean_body": "ë³¸ë¬¸ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ë¶™ì¸ í…ìŠ¤íŠ¸",
  "reason": "ì„ íƒ ê·¼ê±° í•œ ì¤„"
}}

[ì…ë ¥ ì •ë³´]
- ì œëª©: {title}
- URL: {url or ""}
- í›„ë³´ ë¬¸ë‹¨ ë¦¬ìŠ¤íŠ¸(JSON): {blocks_json}

ê·œì¹™:
- clean_bodyì—ëŠ” ê¸°ì‚¬ ë³¸ë¬¸ ë¬¸ì¥ë§Œ ë‚¨ê²¨ë¼. ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤‘ë³µì„ ì—†ì• ê³  ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ê°œë§Œ ë‘”ë‹¤.
- ê³µìœ /ìŠ¤í¬ë©/ì¸ì‡„/ê¸€ìí¬ê¸°/ë‰´ìŠ¤ ì œê³µ/ì €ì‘ê¶Œ/êµ¬ë…/ëŒ“ê¸€/ê´€ë ¨ê¸°ì‚¬/ê´‘ê³  ë“± UI í…ìŠ¤íŠ¸ëŠ” ëª¨ë‘ ì œê±°.
- ë³¸ë¬¸ì´ í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ is_article=falseë¡œ í•˜ê³  reasonë§Œ ì±„ìš´ë‹¤.
""".strip()


async def _llm_select_article_body(title: str, url: str | None, body: str) -> tuple[str | None, str | None]:
    blocks = _split_paragraphs(body)
    if not blocks:
        return None, "no_blocks"

    prompt = _build_llm_body_prompt(title, url, blocks)
    raw = await call_llm(prompt)
    js = extract_json_block(raw)

    try:
        data = json.loads(js)
    except Exception:
        return None, "json_parse_failed"

    if data.get("is_article") is False:
        return None, str(data.get("reason") or "rejected")

    cleaned = _normalize_body_text(data.get("clean_body") or data.get("body") or "")
    if len(cleaned) < 80:
        return None, "llm_body_too_short"

    return cleaned, str(data.get("reason") or "llm_selected")


async def _select_and_clean_article_body(title: str, url: str | None, raw_body: str) -> tuple[str, str | None]:
    precleaned = _rule_based_article_cleanup(raw_body)
    llm_reason: str | None = None

    try:
        llm_body, llm_reason = await _llm_select_article_body(title, url, precleaned)
    except Exception as exc:
        logger.info("[body_clean] llm failed for url=%s err=%s", url, exc)
        llm_body = None

    final_body = llm_body or precleaned or (raw_body or "")
    return final_body, llm_reason


async def _fetch_and_save_article(
    item: dict[str, Any],
    timeout_ms: int,
    semaphore: asyncio.Semaphore,
) -> tuple[dict[str, str] | None, dict[str, str] | None]:
    """
    Fetch article via Playwright MCP and save to WEBRESEARCH_OUT_DIR.
    Returns (saved, failed) where each is a dict.
    """
    link = item.get("link") or item.get("url")
    title = item.get("title") or ""
    if not link:
        return None, {"link": "", "reason": "invalid_link"}

    async with semaphore:
        data, err = await _call_playwright_fetch(link, timeout_ms)
        if err or data is None:
            return None, {"link": link, "reason": f"playwright_failed: {err or 'no data'}"}

        link2, title2, body = _extract_article_payload(data)
        final_link = link2 or link
        final_title = (title2 or title or "").strip()
        if not body:
            return None, {"link": final_link, "reason": "no_body"}

        cleaned_body, llm_reason = await _select_and_clean_article_body(final_title, final_link, body)
        if llm_reason:
            logger.info("[body_clean] llm_reason=%s url=%s", llm_reason, final_link)

        try:
            file_path = save_txt(WEBRESEARCH_OUT_DIR, final_title, final_link, cleaned_body)
        except Exception as exc:
            return None, {"link": final_link, "reason": f"save_failed: {exc}"}

    return {"link": final_link, "title": final_title, "file_path": file_path}, None


async def _search_google_news_rss(query: str, max_results: int) -> tuple[list[dict[str, str]], str | None]:
    url = (
        "https://news.google.com/rss/search?"
        f"q={quote_plus(query)}&hl=ko&gl=KR&ceid=KR:ko"
    )
    try:
        async with httpx.AsyncClient(timeout=20.0) as client:
            resp = await client.get(url)
            resp.raise_for_status()
            text = resp.text
    except Exception as exc:
        return [], f"http error: {exc}"

    try:
        import xml.etree.ElementTree as ET

        root = ET.fromstring(text)
        items: list[dict[str, str]] = []
        for item in root.findall(".//item"):
            title_el = item.find("title")
            link_el = item.find("link")
            title = title_el.text.strip() if title_el is not None and title_el.text else ""
            link = link_el.text.strip() if link_el is not None and link_el.text else ""
            if link and title:
                items.append({"link": link, "title": title})
            if len(items) >= max_results:
                break
        return items, None
    except Exception as exc:
        return [], f"parse error: {exc}"


async def _search_searxng(query: str, max_results: int) -> tuple[list[dict[str, str]], str | None]:
    if not SEARXNG_URL:
        return [], "SEARXNG_URL not configured"

    params = {"q": query, "format": "json", "engines": "news"}
    try:
        async with httpx.AsyncClient(timeout=20.0) as client:
            resp = await client.get(SEARXNG_URL.rstrip("/") + "/search", params=params)
    except Exception as exc:
        return [], f"http error: {exc}"

    if resp.status_code >= 400:
        return [], f"searxng status {resp.status_code}"

    try:
        data = resp.json()
    except Exception as exc:
        return [], f"invalid json: {exc}"

    results = []
    for item in data.get("results", []):
        link = item.get("url") or item.get("link")
        title = item.get("title") or ""
        if link and title:
            results.append({"link": link, "title": title})
        if len(results) >= max_results:
            break

    return results, None


@app.post("/api/plan/from-prompt", response_model=PlanGenerateEnvelope)
async def plan_from_prompt(body: PlanGeneratePromptOnlyRequest):
    """
    Open WebUIì—ì„œ ìì—°ì–´ í•œ ì¤„(prompt)ë§Œ ë³´ë‚´ë„
    - ë‚ ì§œ/í‚¤ì›Œë“œ/í˜•ì‹ ë“±ì„ ìë™ ì¶”ì¶œí•œ ë’¤
    - ë©€í‹°ì†ŒìŠ¤ ë°ì´í„° ë¡œë”© â†’ mcp-bridge select-and-summarize â†’ ê¸°íšì„œ ìƒì„± íë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    """
    parsed_req = await parse_plan_request(body.prompt)
    # V3.1: from-promptë„ from-data íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•´
    # 1) prompt â†’ JSON íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    # 2) íŒŒì¼ ëª©ë¡ í•„í„°ë§(_load_markdown_entries)
    # 3) mcp-bridge select-and-summarize
    # 4) ê¸°íšì„œ ìƒì„± LLM í˜¸ì¶œ
    data_req = PlanFromDataRequest(
        goal=parsed_req.topic or "ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ ì œì‘",
        include=body.include or None,  # ì—†ìœ¼ë©´ from-data ë‚´ë¶€ì—ì„œ ê¸°ë³¸ 5ì¢… ì‚¬ìš©
        start_date=parsed_req.start_date,
        end_date=parsed_req.end_date,
        keyword=parsed_req.keyword,
        extra_instruction=parsed_req.extra_instruction,
        prompt=body.prompt,  # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì €ì¥/ë¡œê¹…ìš©
    )
    plan = await generate_plan_from_data_internal(data_req)
    return PlanGenerateEnvelope(success=True, message="ok", data=plan, error=None)


@app.post("/api/diary/preview", response_model=DiaryFormatResponse)
async def api_diary_preview(body: DiaryFormatRequest):
    """
    âœ… í¬ë§· ê²°ê³¼ 'ë¯¸ë¦¬ë³´ê¸°' ì „ìš© ì—”ë“œí¬ì¸íŠ¸
    - íŒŒì¼ì„ ì €ì¥í•˜ê±°ë‚˜ .txtë¥¼ ì´ë™í•˜ì§€ ì•ŠìŒ
    - ê·¸ëƒ¥ LLM ê²°ê³¼ë§Œ ë°˜í™˜
    - Open WebUI / ë³„ë„ Web UI ì—ì„œ ë°”ë¡œ í˜¸ì¶œí•´ì„œ í™”ë©´ì— ë³´ì—¬ì£¼ê¸° ìš©ë„
    """
    prompt = build_diary_format_prompt(body)
    md_text = await call_llm(prompt)
    return DiaryFormatResponse(result=md_text)


def build_diary_repair_prompt(original_md: str) -> str:
    """
    ê¸°ì¡´ì— ì €ì¥ëœ ì¼ê¸° Markdownì„ ì…ë ¥ë°›ì•„:
    - YAML ë©”íƒ€ë°ì´í„°ë¥¼ ì ê²€/ë³´ì™„
    - ëˆ„ë½ëœ í•„ë“œë¥¼ ì±„ìš°ê³ , ì´ìƒí•œ ê°’ì€ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •
    - ë³¸ë¬¸ ì„¹ì…˜ êµ¬ì¡°ëŠ” ìœ ì§€í•˜ë˜, ì•½ê°„ì˜ ë‹¤ë“¬ê¸°ëŠ” í—ˆìš©
    - ì´ë¯¸ ì¡´ì¬í•˜ëŠ” mood, mood_score ê°’ì€ ë®ì–´ì“°ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€
    """
    return f"""
ë‹¹ì‹ ì€ 'ì¼ê¸° ë©”íƒ€ë°ì´í„° ê²€ìˆ˜/ë³´ì • ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.

ì…ë ¥: ì‚¬ìš©ìì˜ ê¸°ì¡´ ì¼ê¸° Markdown (YAML + ë³¸ë¬¸)

ì‘ì—… ëª©í‘œ:
1. YAML front-matterë¥¼ ì ê²€í•˜ê³  ì•„ë˜ í•„ë“œë“¤ì„ ëª¨ë‘ ì˜¬ë°”ë¥´ê²Œ ì±„ìš°ì„¸ìš”.
ì´ë•Œ YAML fornt-matterê°€ íŒŒì¼ì˜ ìµœìƒë‹¨ì— ì˜¤ë„ë¡ ìˆ˜ì •í•˜ì„¸ìš”.
   - date: YYYY-MM-DD í˜•ì‹
   - time: \"HH:MM\" í˜•ì‹ (ë¬¸ìì—´)
   - title: ìì—°ìŠ¤ëŸ½ì§€ë§Œ ê³¼í•˜ì§€ ì•Šê²Œ
   - mood: ì˜ì–´ ì†Œë¬¸ì ìŠ¤ë„¤ì´í¬ì¼€ì´ìŠ¤ (ì˜ˆ: mixed_hopeful, deeply_tired, calm, anxious_relief)
   - mood_score: -1.0 ~ +1.0 ì‹¤ìˆ˜
   - tags: ì¼ê¸° ë‚´ìš©ì„ ê°€ì¥ ì˜ ëŒ€í‘œí•˜ëŠ” í•œêµ­ì–´ í‚¤ì›Œë“œ 3~7ê°œ
   - people: ë“±ì¥ ì¸ë¬¼/ê´€ê³„ ë¦¬ìŠ¤íŠ¸
   - location: home, office, cafe, outdoor ë“±
   - type: diary
   - projects: [\"ì†Œì„¤ì•„ì´ë””ì–´\",\"NGO\",\"IT\",\"ê°€ì¡±\"] ì¤‘ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ê²ƒ 1~2ê°œ
   - scene_potential: ì†Œì„¤ ì¥ë©´ìœ¼ë¡œ ì“¸ ë§Œí•˜ë©´ true, ì•„ë‹ˆë©´ false
   - summary: ì´ ì¼ê¸°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œ í•œêµ­ì–´ ë¬¸ì¥
   - mood / mood_score ê°€ ì´ë¯¸ YAMLì— ìˆì„ ê²½ìš° ê°’ì€ ì ˆëŒ€ ë³€ê²½í•˜ê±°ë‚˜ ì‚­ì œí•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ë‘”ë‹¤.
     ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œë§Œ ìƒˆë¡œ ì¶”ì •í•˜ì—¬ ì¶”ê°€í•œë‹¤.

2. ë³¸ë¬¸ ì„¹ì…˜:
   - # ì˜¤ëŠ˜ ìš”ì•½ (3ì¤„)
   - # ì˜¤ëŠ˜ì˜ ì‚¬ê±´
   - # ê°ì • / ìƒê°
   - # ë°°ìš´ ê²ƒ / í†µì°°
   - # ì†Œì„¤ ì•„ì´ë””ì–´ ë©”ëª¨ (ì˜µì…˜)
   - # TODO / ë‹¤ìŒì— ì´ì–´ì„œ ì“¸ ê²ƒ (ì˜µì…˜)
   ì´ ì„¹ì…˜ êµ¬ì¡°ëŠ” ìœ ì§€í•˜ë˜, ë‚´ìš©ì´ ë„ˆë¬´ ë¹ˆì•½í•˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ê¸ˆ ë³´ì™„í•´ë„ ë©ë‹ˆë‹¤.

3. â€œì…ë ¥ì— YAMLì´ ìˆë”ë¼ë„ ìµœì¢… ê²°ê³¼ëŠ” YAML front-matterë¥¼ ì˜¤ì§ 1ê°œë§Œ ë§Œë“¤ê³ , ë‚˜ë¨¸ì§€ YAML ë¸”ë¡ì€ ì ˆëŒ€ ë‚¨ê¸°ì§€ ë§ˆì„¸ìš”.â€

4. â€œì…ë ¥ì— â€˜ì°¸ê³ ìš© ì›ë¬¸(ì¶œë ¥ ê¸ˆì§€)â€™ ì„¹ì…˜ì´ ìˆë”ë¼ë„, ìµœì¢… ì¶œë ¥ì—ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ê³  ì œê±°í•˜ì„¸ìš”.â€

ì•„ë˜ëŠ” ì‚¬ìš©ìê°€ ì €ì¥í•œ ê¸°ì¡´ Markdownì…ë‹ˆë‹¤.
ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ„ ê·œì¹™ì— ë§ëŠ” ì™„ì„±ëœ Markdown ì „ì²´ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.

----- ê¸°ì¡´ Markdown ì‹œì‘ -----
{original_md}
----- ê¸°ì¡´ Markdown ë -----
""".strip()


def build_data_repair_prompt(original_md: str, doc_type: str) -> str:
    """
    idea/work/web_research/bible ìš© ë²”ìš© YAML ë³´ì • í”„ë¡¬í”„íŠ¸.
    - diary ì „ìš© í•„ë“œ(mood/mood_score)ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤.
    - ê¸°ì¡´ í•„ë“œëŠ” ì‚­ì œ ê¸ˆì§€, ì—†ìœ¼ë©´ ì¶”ê°€ë§Œ.
    """
    return f"""
ë‹¹ì‹ ì€ 'Markdown ë©”íƒ€ë°ì´í„° ê²€ìˆ˜/ë³´ì • ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
ì…ë ¥: ì‚¬ìš©ìì˜ ê¸°ì¡´ Markdown (YAML + ë³¸ë¬¸)

ëª©í‘œ:
1) YAML front-matterë¥¼ íŒŒì¼ ìµœìƒë‹¨ì— ë°°ì¹˜í•˜ê³  ì•„ë˜ í•„ë“œë¥¼ ëª¨ë‘ ì±„ìš°ì„¸ìš”.
   - ì¶œë ¥ì€ ë°˜ë“œì‹œ '---'ë¡œ ì‹œì‘í•˜ëŠ” YAML í”„ë¡ íŠ¸ë§¤í„°ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. ì–´ë–¤ ì„¤ëª… ë¬¸êµ¬ë„ YAML ìœ„ì— ë„£ì§€ ë§ˆì„¸ìš”.
   - type: "{doc_type}"
   - title: ìì—°ìŠ¤ëŸ½ê²Œ ì •ë¦¬ (ì—†ìœ¼ë©´ ë³¸ë¬¸/íŒŒì¼ëª…ì—ì„œ ì¶”ì •)
   - created_at / updated_at: ISO datetime. ê¸°ì¡´ ê°’ì´ ìˆìœ¼ë©´ ìœ ì§€, ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê° ë˜ëŠ” ë¬¸ë§¥ì—ì„œ ì¶”ì •
   - tags: í•µì‹¬ í‚¤ì›Œë“œ 3~7ê°œ ë¦¬ìŠ¤íŠ¸
   - topics: ì£¼ì œ 2~4ê°œ ë¦¬ìŠ¤íŠ¸
   - people: ë“±ì¥ ì¸ë¬¼/ê´€ê³„ ë¦¬ìŠ¤íŠ¸
   - locations: ìœ„ì¹˜/ê³µê°„ ê´€ë ¨ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
   - source: ì›ë¬¸ ì¶œì²˜(URL/ì„œì  ë“±) ë˜ëŠ” null
   - usage: ì´ ë¬¸ì„œì˜ ìš©ë„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["planning"], ["reference"], ["critique"])
   - summary: ë³¸ë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ (ì´ë¯¸ ìˆìœ¼ë©´ ìœ ì§€)
   - ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¶”ê°€/ì»¤ìŠ¤í…€ í•„ë“œëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€
   - diary ì „ìš© í•„ë“œ(mood, mood_score)ëŠ” ì´ ë¬¸ì„œ íƒ€ì…ì—ì„  ì¶”ê°€/ìˆ˜ì •í•˜ì§€ ë§ ê²ƒ

2) ë³¸ë¬¸ êµ¬ì¡°:
   - ê¸°ì¡´ ë³¸ë¬¸ ì„¹ì…˜ì€ ìµœëŒ€í•œ ìœ ì§€
   - ë‚´ìš©ì´ ë¹ˆì•½í•˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì™„ ê°€ëŠ¥í•˜ë‚˜, ì›ë³¸ ì˜ë¯¸ë¥¼ ê³¼ë„í•˜ê²Œ ë³€í˜•í•˜ì§€ ë§ ê²ƒ

3) ì½”ë“œ ë¸”ëŸ­/ì¸ìš©ë¬¸ ë“± ì›ë³¸ í…ìŠ¤íŠ¸ëŠ” í›¼ì†í•˜ì§€ ë§ˆì„¸ìš”.
   - ì´ë¯¸ ì›ë³¸ í…ìŠ¤íŠ¸ ì½”ë“œë¸”ëŸ­(ì˜ˆ: ```text ... ```)ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ì„¸ìš”.
   - ì—†ë‹¤ë©´ ë¬¸ì„œ ë§¨ ì•„ë˜ì— "ì›ë³¸ í…ìŠ¤íŠ¸ (ìë™ ë³´ì¡´)" ì„¹ì…˜ì„ ë§Œë“¤ê³  ```text ì½”ë“œë¸”ëŸ­``` ì•ˆì— ì…ë ¥ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë„£ìœ¼ì„¸ìš”.

4) ì¶œë ¥ í˜•ì‹:
   - ì„¤ëª…/í•´ì„¤/ì£¼ì„ ì—†ì´ ìµœì¢… Markdownë§Œ ì¶œë ¥í•˜ì„¸ìš”.
   - YAML í”„ë¡ íŠ¸ë§¤í„° ë°”ë¡œ ë’¤ì— ë³¸ë¬¸ì„ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.

----- ê¸°ì¡´ Markdown ì‹œì‘ -----
{original_md}
----- ê¸°ì¡´ Markdown ë -----
""".strip()


@app.post("/api/diary/reformat-md", response_model=DiaryReformatResponse)
async def api_diary_reformat_md(body: DiaryReformatRequest):
    prompt = build_diary_repair_prompt(body.markdown)

    new_md = await call_llm_with_front_matter_retry(
        prompt=prompt,
        validate_meta_fn=validate_diary_front_matter,
        retries=int(os.environ.get("LLM_VALIDATE_RETRIES", "2")),
        must_have_front_matter=True,
    )
    return DiaryReformatResponse(result=new_md)

@app.post("/api/data/reformat-md")
async def api_data_reformat_md(body: DataReformatRequest):
    """
    ë²”ìš© ë°ì´í„°(md) YAML ë³´ì • API.
    - doc_type: idea | work | web_research | bible
    - diary ì „ìš© mood/mood_scoreëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ.
    """
    allowed = {"idea", "work", "web_research", "bible"}
    doc_type = body.doc_type.strip().lower()
    if doc_type not in allowed:
        return standard_response(
            success=False,
            message="invalid doc_type",
            data=None,
            error=f"doc_type must be one of {sorted(allowed)}",
        )


    prompt = build_data_repair_prompt(body.markdown, doc_type)
    try:
        new_md = await call_llm_with_front_matter_retry(
            prompt=prompt,
            validate_meta_fn=lambda meta: validate_data_front_matter(meta, doc_type),
            retries=int(os.environ.get("LLM_VALIDATE_RETRIES", "2")),
            must_have_front_matter=True,
        )
        return standard_response(success=True, message="ok", data={"result": new_md}, error=None)
    except Exception as exc:
        return standard_response(
            success=False,
            message="reformat failed",
            data=None,
            error=str(exc),
        )


# =========================
# ğŸŒ Web UI ë¼ìš°íŠ¸
# =========================


@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    return RedirectResponse(url="/dashboard", status_code=302)


@app.get("/dashboard", response_class=HTMLResponse)
async def dashboard(
    request: Request,
    start_date: str | None = None,
    end_date: str | None = None,
):
    mood_stats = await get_mood_stats(start_date=start_date, end_date=end_date)
    data_roots = {
        "diary": DIARY_ROOT,
        "ideas": IDEAS_ROOT,
        "works": WORKS_ROOT,
        "bible": BIBLE_ROOT,
        "web_research": WEB_RESEARCH_ROOT,
    }
    data_files = {
        name: _list_files_under(path, limit=MAX_FILES_PER_TYPE * 30)
        for name, path in data_roots.items()
    }

    return templates.TemplateResponse(
        "dashboard.html",
        {
            "request": request,
            "user": os.environ.get("DASHBOARD_USER", "guest"),
            "mood_stats": mood_stats,
            "data_roots": data_roots,
            "data_files": data_files,
        },
    )


@app.get("/llm-info")
async def llm_info():
    return {
        "backend": LLM_BACKEND,
        "model": MODEL_NAME,
        "ollama_url": OLLAMA_URL if LLM_BACKEND == "ollama" else None,
        "openai_model": OPENAI_MODEL if LLM_BACKEND == "openai" else None,
    }


# =========================
# ğŸ“š ë°ì´í„° ê¸°ë°˜ ê¸°íšì„œ ìƒì„± (NEW)
# =========================

DATA_ROOTS = {
    "diary": Path(DIARY_ROOT),
    "ideas": Path(IDEAS_ROOT),
    "web_research": Path(WEB_RESEARCH_ROOT),
    "works": Path(WORKS_ROOT),
    "bible": Path(BIBLE_ROOT),
}


def _split_front_matter(text: str) -> tuple[dict[str, Any], str]:
    """
    ê°„ë‹¨í•œ YAML front-matter íŒŒì„œ. ì—†ìœ¼ë©´ ({}, ì „ì²´ í…ìŠ¤íŠ¸) ë°˜í™˜.
    """
    if not text.lstrip().startswith("---"):
        return {}, text
    parts = text.split("---", 2)
    if len(parts) < 3:
        return {}, text
    meta_raw = parts[1]
    body = parts[2]
    try:
        meta = yaml.safe_load(meta_raw) or {}
    except Exception:
        meta = {}
    return meta, body


def _ensure_list(val: Any) -> list[str]:
    if val is None:
        return []
    if isinstance(val, list):
        return [str(v) for v in val]
    return [str(val)]


def _parse_meta_date(meta: dict[str, Any]) -> date | None:
    for key in ("date", "created_at", "updated_at"):
        val = meta.get(key)
        if isinstance(val, datetime):
            return val.date()
        if isinstance(val, date):
            return val
        if isinstance(val, str):
            try:
                return datetime.fromisoformat(val).date()
            except Exception:
                continue
    return None


def _guess_date_from_filename(path: Path) -> date | None:
    name = path.stem
    candidates = [name[:10], name.split("_")[0]]
    for cand in candidates:
        try:
            return datetime.fromisoformat(cand).date()
        except Exception:
            pass
    digits = "".join(ch for ch in name if ch.isdigit())
    if len(digits) >= 8:
        try:
            return datetime.strptime(digits[:8], "%Y%m%d").date()
        except Exception:
            pass
    return None


def _matches_filter(meta: dict[str, Any], body: str, rel_path: str, start_date: str | None, end_date: str | None, keyword: str | None) -> bool:
    def parse_date_safe(s: str | None):
        if not s:
            return None
        try:
            return datetime.fromisoformat(s).date()
        except Exception:
            return None

    diary_date = _parse_meta_date(meta) or _guess_date_from_filename(Path(rel_path))

    if start_date:
        s = parse_date_safe(start_date)
        if s and diary_date and diary_date < s:
            return False
    if end_date:
        e = parse_date_safe(end_date)
        if e and diary_date and diary_date > e:
            return False

    if keyword:
        meta_fields = (
            [meta.get("title") or ""]
            + _ensure_list(meta.get("tags"))
            + _ensure_list(meta.get("topics"))
            + _ensure_list(meta.get("people"))
            + _ensure_list(meta.get("locations"))
        )
        in_meta = any(keyword in t for t in meta_fields)
        if (not in_meta) and (keyword not in body):
            return False
    return True


def _load_markdown_entries(kind: str, start_date: str | None, end_date: str | None, keyword: str | None, limit: int = MAX_FILES_PER_TYPE) -> list[dict[str, Any]]:
    root = DATA_ROOTS.get(kind)
    if not root or not root.exists():
        return []

    files = sorted(root.glob("*.md"), key=lambda p: p.stat().st_mtime, reverse=True)
    entries: list[dict[str, Any]] = []

    for path in files:
        if len(entries) >= limit:
            break
        text = path.read_text(encoding="utf-8", errors="ignore")
        meta, body = _split_front_matter(text)

        if not _matches_filter(meta, body, str(path.relative_to(root)), start_date, end_date, keyword):
            continue

        entry = {
            "path": str(path),
            "rel_path": str(path.relative_to(root)),
            "title": meta.get("title") or path.stem,
            "tags": _ensure_list(meta.get("tags")),
            "topics": _ensure_list(meta.get("topics")),
            "summary": meta.get("summary") or "",
            "content": body.strip(),
            "meta": meta,
        }
        entries.append(entry)

    return entries


def _render_section(label: str, entries: list[dict[str, Any]]):
    if not entries:
        return f"[{label}]\n- (ë°ì´í„° ì—†ìŒ)\n"
    lines = [f"[{label}]"]
    for e in entries:
        meta = e.get("meta", {})
        tags = _ensure_list(meta.get("tags") or e.get("tags"))
        topics = _ensure_list(meta.get("topics") or e.get("topics"))
        meta_info = []
        if tags:
            meta_info.append(f"tags={','.join(tags)}")
        if topics:
            meta_info.append(f"topics={','.join(topics)}")
        meta_str = f" ({'; '.join(meta_info)})" if meta_info else ""
        title = e.get("title") or (meta.get("title") or e.get("rel_path"))
        rel_path = e.get("rel_path") or e.get("path") or ""
        lines.append(f"## {title} [{rel_path}] {meta_str}".strip())
        summary_line = meta.get("summary") or e.get("summary")
        if summary_line:
            lines.append(f"- summary: {summary_line}")
        content = (
            e.get("content")
            or e.get("excerpt")
            or e.get("body")
            or ""
        ).strip()
        excerpt = content[:1200]
        if len(content) > 1200:
            excerpt += "\n...[ë³¸ë¬¸ ê¸¸ì´ ì´ˆê³¼ë¡œ ì¼ë¶€ë§Œ í¬í•¨]"
        if excerpt:
            lines.append(excerpt)
        lines.append("")
    return "\n".join(lines)


def build_plan_prompt_from_data(goal: str, data: dict[str, list[dict[str, Any]]], extra_instruction: str | None = None) -> str:
    """
    ë°ì´í„° ì†ŒìŠ¤ë³„ ì„¹ì…˜ì„ ë¶„ë¦¬í•´ LLMì— ì „ë‹¬.
    WORKS/BIBLE ê·œì¹™ í¬í•¨.
    """
    extra = (extra_instruction or "").strip()

    prompt_parts = [
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë‹¤ì¤‘ ì†ŒìŠ¤(md íŒŒì¼) ë°ì´í„°ë¥¼ ì½ê³  ë‹¨í¸ì†Œì„¤ ê¸°íšì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        "ê° ì„¹ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•˜ê³ , ì‚¬ì‹¤ì„ ì°½ì‘í•˜ì§€ ë§ˆì„¸ìš”.",
        f"[ìš”ì²­ ëª©í‘œ]\n- {goal}",
        "",
        _render_section("DIARY", data.get("diary", [])),
        _render_section("IDEAS", data.get("ideas", [])),
        _render_section("WEB_RESEARCH", data.get("web_research", [])),
        _render_section("WORKS", data.get("works", [])),
        _render_section("BIBLE", data.get("bible", [])),
        "",
        "[ì‘ì„± ê·œì¹™]",
        "- WORKS(ì‘í’ˆ)ì—ì„œ ì•„ì´ë””ì–´ë¥¼ ì°¨ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ í¬í•¨:",
        "  [ì°¸ê³  ì‘í’ˆ: <íŒŒì¼ëª…>]",
        "  - í•´ë‹¹ ì•„ì´ë””ì–´ê°€ ë‚˜ì˜¨ ì´ìœ :",
        "  - ì‘í’ˆì˜ ì´ ì•„ì´ë””ì–´ê°€ ì í•©í•œ ê·¼ê±°:",
        "- BIBLE ë°ì´í„°ë¥¼ ì¸ìš©í•  ë•Œ:",
        "  - ì§ì ‘ ì¸ìš© ì‹œ ë°˜ë“œì‹œ [ì„±ê²½: ì±… ì¥:ì ˆ (ë²ˆì—­)] í˜•ì‹ìœ¼ë¡œ í‘œê¸°",
        "  - ê¸°íš ë°©í–¥/í†µì°° ë³´ì¡°ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì‚¬ëŒ/ì‘í’ˆì„ ë‹¨ì •/ì‹¬íŒí•˜ëŠ” í‘œí˜„ì€ ê¸ˆì§€",
        "- ì„¹ì…˜ë³„ ì›ë¬¸ ê·¼ê±°ë¥¼ ì¸ìš©í•˜ë©°, ì—†ëŠ” ê²½ìš° 'ê·¼ê±° ë¶€ì¡±'ì„ ëª…ì‹œ",
        "- ê¸°ì¡´ ê¸°íšì„œ í†¤ì„ ìœ ì§€í•˜ë˜ ë°ì´í„° ê·¼ê±° ìš°ì„ ìœ¼ë¡œ ì‘ì„±",
    ]

    if extra:
        prompt_parts.append(f"- ì¶”ê°€ ì§€ì‹œ: {extra}")

    return "\n".join(prompt_parts)


async def _apply_prompt_to_data_request(req: PlanFromDataRequest) -> PlanFromDataRequest:
    """
    plan/from-data ì „ìš©: promptê°€ ë“¤ì–´ì˜¤ë©´ parse_plan_requestë¥¼ í†µí•´
    start_date/end_date/keyword/goal(extra) ë¥¼ ì±„ì›Œë„£ëŠ”ë‹¤.
    ê¸°ì¡´ í•„ë“œê°€ ì´ë¯¸ ì£¼ì–´ì¡Œë‹¤ë©´ ë®ì–´ì“°ì§€ ì•ŠëŠ”ë‹¤.
    """
    if not req.prompt:
        return req
    try:
        parsed = await parse_plan_request(req.prompt)
    except Exception:
        return req

    updates: dict[str, Any] = {}
    if (not req.start_date) and parsed.start_date:
        updates["start_date"] = parsed.start_date
    if (not req.end_date) and parsed.end_date:
        updates["end_date"] = parsed.end_date
    if (not req.keyword) and parsed.keyword:
        updates["keyword"] = parsed.keyword
    if (not req.extra_instruction) and parsed.extra_instruction:
        updates["extra_instruction"] = parsed.extra_instruction
    # goalì€ ê¸°ë³¸ê°’ì¼ ë•Œë§Œ topicìœ¼ë¡œ ëŒ€ì²´
    if (req.goal == "ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ ì œì‘" or not req.goal) and parsed.topic:
        updates["goal"] = parsed.topic
    return req.copy(update=updates)


def _render_sources_list(sources: dict[str, list[str]]) -> str:
    """
    ê¸°íšì„œ ê²°ê³¼ë¬¼ì— ì‚¬ëŒì´ ë°”ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì°¸ê³  íŒŒì¼ ëª©ë¡ì„ ì¶”ê°€.
    """
    if not sources:
        return "## ì°¸ê³  ì†ŒìŠ¤ ëª©ë¡\n- (ì—†ìŒ)"
    lines = ["## ì°¸ê³  ì†ŒìŠ¤ ëª©ë¡"]
    for kind, items in sources.items():
        if not items:
            lines.append(f"- {kind}: (ì—†ìŒ)")
            continue
        for path in items:
            lines.append(f"- {kind}: {path}")
    return "\n".join(lines)

def build_plan_header_yaml(title: str, goal: str, include: list[str], sources: dict[str, list[str]]) -> str:
    now_iso = datetime.now().isoformat()
    header = {
        "type": "plan",
        "title": title,
        "goal": goal,
        "include": include,
        "created_at": now_iso,
        "updated_at": now_iso,
        "usage": ["planning"],
        "sources": sources,   # âœ… validate_plan_front_matterì˜ sources ìš”êµ¬ ì¶©ì¡±
    }
    return "---\n" + yaml.safe_dump(header, allow_unicode=True, sort_keys=False) + "---\n\n"

async def generate_plan_from_data_internal(req: PlanFromDataRequest) -> PlanGenerateResponse:
    req = await _apply_prompt_to_data_request(req)
    includes = req.include or ["diary", "ideas", "web_research", "works", "bible"]
    includes = [i for i in includes if i in DATA_ROOTS]
    topic = req.goal or "ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ ì œì‘"

    # mcp-bridge ìƒˆ select-and-summarize ì‚¬ìš©: íŒŒì¼ ì„ íƒ + ìš”ì•½
    selection = await select_and_summarize(
        include=includes,
        start_date=req.start_date,
        end_date=req.end_date,
        keyword=req.keyword,
        extra_instruction=req.extra_instruction,
        limit_per_type=MAX_FILES_PER_TYPE,
        preview_chars=1200,
    )

    filtered_data = selection.get("entries", {})
    summary_text = selection.get("result", "")
    sources_dict = selection.get("sources", {})

    # ê¸°ì¡´ build_plan_prompt í…œí”Œë¦¿(1~10 ì„¹ì…˜ + í†¤/ìŠ¤íƒ€ì¼)ì„ ê·¸ëŒ€ë¡œ í™œìš©
    sections = []
    for kind in includes:
        label = kind.upper()
        entries = filtered_data.get(kind, [])
        sections.append(_render_section(label, entries))
    multi_source_summary = "\n\n".join(sections)
    multi_source_summary = multi_source_summary + "\n\n[ë©€í‹°ì†ŒìŠ¤ ìš”ì•½]\n" + summary_text

    prompt = build_plan_prompt(
        topic=topic,
        diary_summary=multi_source_summary,
        extra_instruction=req.extra_instruction,
    )

    plan_text = await call_llm(prompt)

    # (ì„ íƒ) ë³¸ë¬¸ í’ˆì§ˆ ê·œì¹™: ë„ˆë¬´ ì§§ìœ¼ë©´ ì¬ì‹œë„ ê°™ì€ ê°„ë‹¨ ê·œì¹™
    if len(plan_text.strip()) < 500:
        # ì¬ì‹œë„: (í”„ë¡¬í”„íŠ¸ ê°•í™”)
        plan_text = await call_llm(prompt + "\n\n[í’ˆì§ˆ ê¸°ì¤€] ìµœì†Œ 500ì ì´ìƒ, ì„¹ì…˜ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ì±„ì›Œë¼.")

    sources_section = _render_sources_list(sources_dict)
    plan_text_with_sources = f"{plan_text}\n\n---\n{sources_section}"

    header_str = build_plan_header_yaml(
        title=req.goal or "ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ ì œì‘",
        goal=req.goal or "ë‹¨í¸ì†Œì„¤ ê¸°íšì„œ ì œì‘",
        include=includes,
        sources=sources_dict,
    )
    final_text = header_str + plan_text_with_sources

    final_text = header_str + plan_text_with_sources

    # âœ… ìµœì¢… ê²°ê³¼(front-matter í¬í•¨) ê²€ì¦
    meta, _, has_fm = extract_front_matter(final_text)
    if not has_fm:
        raise ValueError("plan output missing front-matter (server bug)")
    validate_plan_front_matter(meta)

    file_path = save_plan_output(final_text)

    return PlanGenerateResponse(
        title=req.goal,
        content=final_text,
        file_path=file_path,
        sources=[sources_dict],
    )


@app.post("/api/plan/from-data", response_model=PlanGenerateEnvelope)
async def plan_from_data(req: PlanFromDataRequest):
    """
    ë‹¤ì¤‘ ë°ì´í„° ì†ŒìŠ¤(diary/ideas/web_research/works/bible)ë¥¼ ì„¹ì…˜ë³„ë¡œ LLMì— ì „ë‹¬í•´ ë‹¨í¸ì†Œì„¤ ê¸°íšì„œë¥¼ ìƒì„±í•œë‹¤.
    OpenWebUI HTTP Tool ì„¤ì • ì˜ˆì‹œ:
    - Method: POST
    - URL: http://waai-backend:8000/api/plan/from-data
    - Headers: Content-Type: application/json
    - Body ì˜ˆì‹œ:
      {
        "prompt": "12ì›” ê°€ì¡± ì¼ê¸° ê¸°ë°˜ìœ¼ë¡œ ë”°ëœ»í•œ ê°ë™ ë‹¨í¸ ê¸°íšì„œ. 12/1~12/31 ì‚¬ì´ ê¸°ë¡ë§Œ, í¬ë§ì  ê²°ë§.",
        "goal": "ìµœê·¼ ì¼ê¸° ê¸°ë°˜ ë‹¨í¸ì†Œì„¤ ê¸°íš",
        "include": ["diary", "ideas", "web_research", "works", "bible"],
        "start_date": "2025-12-01",
        "end_date": "2025-12-31",
        "keyword": "ê°€ì¡±",
        "extra_instruction": "í¬ë§ì  ê²°ë§ë¡œ ë§ˆë¬´ë¦¬"
      }
    - ì„±ê³µ ì‘ë‹µ ì˜ˆì‹œ:
      {
        "success": true,
        "message": "ok",
        "data": {
          "title": "...",
          "content": "...(ê¸°íšì„œ ë³¸ë¬¸)...",
          "file_path": "/data/outputs/202512xx_plan.md",
          "sources": [{"diary": ["2025-12-10.md"], "ideas": ["foo.md"]}]
        },
        "error": null
      }
    - ì‹¤íŒ¨ ì‘ë‹µ ì˜ˆì‹œ:
      {
        "success": false,
        "message": "plan generation failed",
        "data": null,
        "error": "ì—ëŸ¬ ë©”ì‹œì§€"
      }

    curl í…ŒìŠ¤íŠ¸:
    curl -X POST http://waai-backend:8000/api/plan/from-data \\
      -H "Content-Type: application/json" \\
      -d '{"prompt":"12ì›” ê°€ì¡± ì¼ê¸° ê¸°ë°˜ ê¸°íšì„œ","include":["diary","ideas"],"keyword":"ê°€ì¡±"}'
    """
    try:
        plan = await generate_plan_from_data_internal(req)
        return PlanGenerateEnvelope(
            success=True,
            message="ok",
            data=plan,
            error=None,
        )
    except Exception as exc:
        return PlanGenerateEnvelope(
            success=False,
            message="plan generation failed",
            data=None,
            error=str(exc),
        )


def _list_files_under(root: str | Path, limit: int = 200) -> list[dict[str, Any]]:
    """
    /data ë‚´ë¶€ íŒŒì¼ì„ ìµœì‹  ìˆ˜ì •ìˆœìœ¼ë¡œ ì ë‹¹íˆ ë³´ì—¬ì£¼ê¸° ìœ„í•œ í—¬í¼.
    """
    base = Path(root)
    if not base.exists():
        return []

    files: list[tuple[float, Path]] = []
    for path in base.rglob("*"):
        if path.is_file():
            try:
                mtime = path.stat().st_mtime
            except OSError:
                continue
            files.append((mtime, path))

    files.sort(key=lambda x: x[0], reverse=True)
    items: list[dict[str, Any]] = []

    for mtime, path in files[:limit]:
        rel = str(path.relative_to(base))
        try:
            size_kb = round(path.stat().st_size / 1024, 1)
        except OSError:
            size_kb = None
        items.append(
            {
                "rel_path": rel,
                "name": path.name,
                "mtime": datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M"),
                "size_kb": size_kb,
            }
        )
    return items


# =========================
# ğŸŒ Playwright ì›¹ ë¦¬ì„œì¹˜ API
# =========================


@app.post("/api/playwright/crawl", response_model=dict, operation_id="playwright_crawl")
async def playwright_crawl(req: PlaywrightCrawlRequest):
    """
    OpenWebUI ì»¤ìŠ¤í…€ íˆ´ì—ì„œ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” Playwright í¬ë¡¤ë§ ì—”ë“œí¬ì¸íŠ¸.
    - urlì´ ì£¼ì–´ì§€ë©´ Playwright MCPì˜ fetch ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•´ ë‹¨ì¼ í˜ì´ì§€ ë³¸ë¬¸ì„ ë°˜í™˜
    - (í˜¸í™˜ì„±) ê¸°ì¡´ prompt/keywords ì…ë ¥ì€ ê¸°ì¡´ í¬ë¡¤ë§ í”Œë¡œìš°ë¡œ ë™ì‘
    """
    if req.url:
        data, err = await _call_playwright_fetch(req.url, req.timeout_ms)
        if err or data is None:
            return standard_response(
                success=False,
                message="playwright fetch failed",
                data=None,
                error=err or "no data",
            )

        link, title, body = _extract_article_payload(data)
        if not body:
            return standard_response(
                success=False,
                message="playwright fetch returned no body",
                data=None,
                error="missing body/text/content",
            )

        return standard_response(
            success=True,
            message="ok",
            data={"link": link or req.url, "title": title or "", "body": body},
            error=None,
        )

    keywords = [k.strip() for k in req.keywords if k and k.strip()]
    if not keywords and req.prompt:
        keywords = _extract_keywords_from_prompt(req.prompt, limit=5)
    keywords = keywords[:5]
    if not keywords:
        return standard_response(success=False, message="keywords required", data=None, error="no keywords")

    per_keyword = max(1, min(req.per_keyword, 5))
    payload = {"keywords": keywords, "perKeyword": per_keyword}
    data = await _call_playwright_crawl(payload)
    if data is None:
        return standard_response(success=False, message="playwright crawl failed", data=None, error="call failed")

    saved = data.get("saved_files") or data.get("savedFiles") or []
    count = data.get("count") or len(saved)
    articles = data.get("articles") or []
    return standard_response(
        success=True,
        message="ok",
        data={"saved_files": saved, "count": count, "keywords": keywords, "articles": articles},
        error=None,
    )


@app.post("/api/web/search", response_model=dict, operation_id="web_search")
async def web_search(req: WebSearchRequest):
    query = normalize_query(req.query)
    if not query:
        return standard_response(success=False, message="query required", data=None, error="empty query")

    max_results = max(1, min(req.max_results or 5, 20))
    engine = (req.engine or "google_news_rss").lower()

    items: list[dict[str, str]] = []
    error: str | None = None

    if engine == "searxng" or (engine != "google_news_rss" and SEARXNG_URL):
        items, error = await _search_searxng(query, max_results)
        engine_used = "searxng"
    else:
        items, error = await _search_google_news_rss(query, max_results)
        engine_used = "google_news_rss"

    if error:
        return standard_response(success=False, message="search failed", data=None, error=error)

    return standard_response(
        success=True,
        message="ok",
        data={"query": query, "engine": engine_used, "items": items[:max_results]},
        error=None,
    )


MAX_PLAYWRIGHT_CONCURRENCY = 2


@app.post("/api/web_search/fetch", response_model=dict, operation_id="web_search_fetch")
async def web_search_fetch(req: WebSearchFetchRequest):
    query = normalize_query(req.query)
    if not query:
        return standard_response(success=False, message="query required", data=None, error="empty query")

    max_results = max(1, min(req.max_results or 5, 20))
    search_result = await web_search(WebSearchRequest(query=query, max_results=max_results, engine=req.engine))

    if not search_result.get("success"):
        return standard_response(
            success=False,
            message="search failed",
            data=None,
            error=search_result.get("error") or "search_failed",
        )

    items = (search_result.get("data") or {}).get("items") or []
    if not items:
        logger.info("[web_search_fetch] no search results for query=%s", query)
        return standard_response(
            success=False,
            message="no search results",
            data=None,
            error="no_search_results",
        )

    items = items[:max_results]
    logger.info("[web_search_fetch] query=%s engine=%s results=%d", query, req.engine, len(items))

    semaphore = asyncio.Semaphore(MAX_PLAYWRIGHT_CONCURRENCY)
    tasks = [
        _fetch_and_save_article(item, req.timeout_ms or 20000, semaphore)
        for item in items
    ]
    results = await asyncio.gather(*tasks)

    saved: list[dict[str, str]] = []
    failed: list[dict[str, str]] = []

    for ok, err in results:
        if ok:
            saved.append(ok)
            logger.info("[web_search_fetch] saved title=%s path=%s", ok.get("title", ""), ok.get("file_path", ""))
        elif err:
            failed.append(err)
            logger.info("[web_search_fetch] failed link=%s reason=%s", err.get("link", ""), err.get("reason", ""))

    message = f"saved {len(saved)} items"
    return standard_response(
        success=bool(saved) or not failed,  # allow partial success
        message=message,
        data={"query": query, "saved": saved, "failed": failed},
        error=None if saved or not failed else "all_failed",
    )


# =========================
# ğŸŒ Playwright ì›¹ë¦¬ì„œì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬
# =========================

_playwright_logs: deque[dict[str, Any]] = deque(maxlen=30)
_playwright_scheduler_task: asyncio.Task | None = None


def _load_playwright_schedule() -> PlaywrightScheduleConfig:
    if PLAYWRIGHT_SCHEDULE_PATH.exists():
        try:
            data = json.loads(PLAYWRIGHT_SCHEDULE_PATH.read_text(encoding="utf-8"))
            return PlaywrightScheduleConfig(**data)
        except Exception:
            pass
    return PlaywrightScheduleConfig()


def _save_playwright_schedule(cfg: PlaywrightScheduleConfig):
    PLAYWRIGHT_SCHEDULE_PATH.write_text(cfg.model_dump_json(ensure_ascii=False, indent=2), encoding="utf-8")


def _log_playwright_run(kind: str, keywords: list[str], count: int, saved: list[str], error: str | None = None):
    _playwright_logs.appendleft(
        {
            "time": datetime.now().isoformat(),
            "kind": kind,
            "keywords": keywords,
            "count": count,
            "saved_files": saved[:5],
            "error": error,
        }
    )


async def _playwright_scheduler_loop():
    while True:
        cfg = _load_playwright_schedule()
        if cfg.enabled and cfg.keywords:
            payload = {
                "keywords": cfg.keywords[:5],
                "perKeyword": max(1, min(cfg.per_keyword, 5)),
            }
            data = await _call_playwright_crawl(payload)
            now_iso = datetime.now().isoformat()
            if data is None:
                cfg.last_run = now_iso
                cfg.last_error = "playwright call failed"
                cfg.last_count = 0
                _log_playwright_run("schedule", payload["keywords"], 0, [], cfg.last_error)
            else:
                saved = data.get("saved_files") or data.get("savedFiles") or []
                count = data.get("count") or len(saved)
                cfg.last_run = now_iso
                cfg.last_error = None
                cfg.last_count = count
                _log_playwright_run("schedule", payload["keywords"], count, saved, None)
            _save_playwright_schedule(cfg)

        interval = max(1, cfg.interval_minutes) * 60
        await asyncio.sleep(interval)


@app.on_event("startup")
async def _start_playwright_scheduler():
    global _playwright_scheduler_task
    if _playwright_scheduler_task is None:
        _playwright_scheduler_task = asyncio.create_task(_playwright_scheduler_loop())


@app.get("/api/playwright/schedule", response_model=dict, operation_id="get_playwright_schedule")
async def get_playwright_schedule():
    cfg = _load_playwright_schedule()
    return standard_response(success=True, message="ok", data=cfg.dict(), error=None)


@app.post("/api/playwright/schedule", response_model=dict, operation_id="set_playwright_schedule")
async def set_playwright_schedule(cfg: PlaywrightScheduleConfig):
    cfg.interval_minutes = max(1, cfg.interval_minutes)
    cfg.per_keyword = max(1, min(cfg.per_keyword, 5))
    cfg.keywords = [k.strip() for k in cfg.keywords if k and k.strip()][:5]
    _save_playwright_schedule(cfg)
    return standard_response(success=True, message="saved", data=cfg.dict(), error=None)


@app.get("/api/playwright/status", response_model=dict, operation_id="get_playwright_status")
async def get_playwright_status():
    return standard_response(success=True, message="ok", data=list(_playwright_logs), error=None)


# =========================
# ğŸ“‘ ë‹¨í¸ì†Œì„¤ í•©í‰ API (NEW)
# =========================

CRITIQUE_FALLBACK_RULES = """- ë“±ì¥ì¸ë¬¼ì˜ ëª©í‘œì™€ ê°ˆë“±ì´ ëšœë ·í•œê°€?
- ì¥ë©´ë§ˆë‹¤ êµ¬ì²´ì  ê°ì •/ê°ê° ë¬˜ì‚¬ê°€ ìˆëŠ”ê°€?
- ì‚¬ê±´ ì§„í–‰ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì´ì–´ì§€ëŠ”ê°€?
- ëŒ€ì‚¬ê°€ ì¸ë¬¼ ì„±ê²©ê³¼ ìƒí™©ì— ë§ëŠ”ê°€?
- ë§ˆë¬´ë¦¬ê°€ ì£¼ì œ ì˜ì‹ê³¼ ì •ì„œì  ì—¬ìš´ì„ ì „ë‹¬í•˜ëŠ”ê°€?
"""


def _load_critique_criteria() -> str:
    if CRITIQUE_CRITERIA_PATH.exists():
        try:
            return CRITIQUE_CRITERIA_PATH.read_text(encoding="utf-8")
        except Exception:
            return CRITIQUE_FALLBACK_RULES
    return CRITIQUE_FALLBACK_RULES


def _save_critique_object(title: str, content: str) -> str:
    now_iso = datetime.now().isoformat()
    yaml_header = {
        "type": "critique_object",
        "title": title,
        "created_at": now_iso,
        "updated_at": now_iso,
        "usage": ["critique", "reference"],
    }
    md = "---\n" + yaml.safe_dump(yaml_header, allow_unicode=True, sort_keys=False) + "---\n\n" + content
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_title = slugify_filename(title)
    path = CRITIQUE_OBJECTS_ROOT / f"{ts}_{safe_title}.md"
    path.write_text(md, encoding="utf-8")
    return str(path)


def _save_critique_result(title: str, critique_text: str, object_path: str) -> str:
    now_iso = datetime.now().isoformat()
    yaml_header = {
        "type": "critique",
        "object_title": title,
        "created_at": now_iso,
        "updated_at": now_iso,
        "source_object_file": object_path,
        "usage": ["critique"],
    }
    md = "---\n" + yaml.safe_dump(yaml_header, allow_unicode=True, sort_keys=False) + "---\n\n" + critique_text
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    path = CRITIQUE_RESULTS_ROOT / f"{ts}_{slugify_filename(title)}_critique.md"
    path.write_text(md, encoding="utf-8")
    return str(path)


def _build_critique_prompt(title: str, content: str, criteria: str, extra_instruction: str | None = None) -> str:
    extra = extra_instruction.strip() if extra_instruction else ""
    extra_block = f"\n\n[ì¶”ê°€ ì§€ì‹œì‚¬í•­]\n{extra}" if extra else ""
    return f"""
ë„ˆëŠ” ë‹¨í¸ì†Œì„¤ í•©í‰ ì „ë¬¸ ì—ë””í„°ë‹¤. ì•„ë˜ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ì§€ì¼œ ì¶œë ¥í•˜ë¼.

[í•©í‰ ê¸°ì¤€]
{criteria}

[ì…ë ¥ ì›ê³  ì œëª©]
{title}

[ì…ë ¥ ì›ê³ ]
{content}
{extra_block}

--- ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ ìˆœì„œë¡œ) ---
(1) í•œ ì¤„ ì´í‰
(2) í•­ëª©ë³„ ì ìˆ˜: ê° í•­ëª© 10ì  ë§Œì , ê·¼ê±°ë¡œ ì›ê³  ë¬¸ì¥/ë¬¸ë‹¨ì„ ì¸ìš©
(3) ê°œì„  ì œì•ˆ: êµ¬ì²´ì ìœ¼ë¡œ ëª‡ ë¼ì¸/ì–´ëŠ ë¬¸ë‹¨ì„ ì–´ë–¤ í‘œí˜„Â·ë¬˜ì‚¬Â·ë°©í–¥ìœ¼ë¡œ ìˆ˜ì •í• ì§€ ì œì•ˆ (ì¥ë©´/ë¬¸ë‹¨ ë‹¨ìœ„)
(4) ê¸°ì¤€ ì¤€ìˆ˜ ì—¬ë¶€ ì²´í¬ë¦¬ìŠ¤íŠ¸: í•©í‰ê¸°ì¤€ê·œì¹™.md í•­ëª©ì„ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ê³  ê° í•­ëª©ì— ëŒ€í•´ ì¤€ìˆ˜/ë¯¸í¡ + í•œ ì¤„ ê·¼ê±°
""".strip()


def _build_critique_chunk_prompt(
    title: str,
    content: str,
    criteria: str,
    part_index: int,
    total_parts: int,
    extra_instruction: str | None = None,
) -> str:
    extra = extra_instruction.strip() if extra_instruction else ""
    extra_block = f"\n\n[ì¶”ê°€ ì§€ì‹œì‚¬í•­]\n{extra}" if extra else ""
    return f"""
ë„ˆëŠ” ë‹¨í¸ì†Œì„¤ í•©í‰ ì „ë¬¸ ì—ë””í„°ë‹¤. ì•„ë˜ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ì§€ì¼œ ì¶œë ¥í•˜ë¼.
ì´ íŒŒíŠ¸ëŠ” ì›ê³ ì˜ ì¼ë¶€ì´ë¯€ë¡œ, ì´ íŒŒíŠ¸ì˜ ë‚´ìš©ì—ë§Œ ê·¼ê±°í•´ í‰ê°€í•˜ë¼.

[í•©í‰ ê¸°ì¤€]
{criteria}

[ì…ë ¥ ì›ê³  ì œëª©]
{title}

[ì…ë ¥ ì›ê³  íŒŒíŠ¸ {part_index}/{total_parts}]
{content}
{extra_block}

--- ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ ìˆœì„œë¡œ) ---
(0) íŒŒíŠ¸ ìš”ì•½: 2~3ë¬¸ì¥ìœ¼ë¡œ í˜„ì¬ íŒŒíŠ¸ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ë²Œì–´ì§€ëŠ”ì§€ ìš”ì•½
(1) í•œ ì¤„ ì´í‰
(2) í•­ëª©ë³„ ì ìˆ˜: ê° í•­ëª© 10ì  ë§Œì , ê·¼ê±°ë¡œ ì›ê³  ë¬¸ì¥/ë¬¸ë‹¨ì„ ì¸ìš©
(3) ê°œì„  ì œì•ˆ: êµ¬ì²´ì ìœ¼ë¡œ ëª‡ ë¼ì¸/ì–´ëŠ ë¬¸ë‹¨ì„ ì–´ë–¤ í‘œí˜„Â·ë¬˜ì‚¬Â·ë°©í–¥ìœ¼ë¡œ ìˆ˜ì •í• ì§€ ì œì•ˆ (ì¥ë©´/ë¬¸ë‹¨ ë‹¨ìœ„)
(4) ê¸°ì¤€ ì¤€ìˆ˜ ì—¬ë¶€ ì²´í¬ë¦¬ìŠ¤íŠ¸: í•©í‰ê¸°ì¤€ê·œì¹™.md í•­ëª©ì„ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ê³  ê° í•­ëª©ì— ëŒ€í•´ ì¤€ìˆ˜/ë¯¸í¡ + í•œ ì¤„ ê·¼ê±°
""".strip()


def _build_critique_overall_prompt(
    title: str,
    part_summaries: list[str],
    criteria: str,
    extra_instruction: str | None = None,
) -> str:
    summary_lines = "\n".join([f"- íŒŒíŠ¸ {idx}: {summary}" for idx, summary in enumerate(part_summaries, start=1)])
    extra = extra_instruction.strip() if extra_instruction else ""
    extra_block = f"\n\n[ì¶”ê°€ ì§€ì‹œì‚¬í•­]\n{extra}" if extra else ""
    return f"""
ë„ˆëŠ” ë‹¨í¸ì†Œì„¤ í•©í‰ ì „ë¬¸ ì—ë””í„°ë‹¤. ì•„ë˜ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ì§€ì¼œ ì¶œë ¥í•˜ë¼.
ì•„ë˜ íŒŒíŠ¸ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ì‘í’ˆ ì „ì²´ì˜ êµ¬ì¡°/ì „ê°œ/ì •ì„œ íë¦„ì„ í‰ê°€í•˜ë¼.

[í•©í‰ ê¸°ì¤€]
{criteria}

[ì…ë ¥ ì›ê³  ì œëª©]
{title}

[íŒŒíŠ¸ ìš”ì•½]
{summary_lines}
{extra_block}

--- ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ ìˆœì„œë¡œ) ---
(1) í•œ ì¤„ ì´í‰
(2) í•­ëª©ë³„ ì ìˆ˜: ê° í•­ëª© 10ì  ë§Œì , ê·¼ê±°ë¡œ íŒŒíŠ¸ ìš”ì•½ì„ ì¸ìš©
(3) ê°œì„  ì œì•ˆ: ì‘í’ˆ ì „ì²´ êµ¬ì¡°/ì „ê°œ/ì •ì„œ íë¦„ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì²´ì  ìˆ˜ì • ë°©í–¥ ì œì•ˆ
(4) ê¸°ì¤€ ì¤€ìˆ˜ ì—¬ë¶€ ì²´í¬ë¦¬ìŠ¤íŠ¸: í•©í‰ê¸°ì¤€ê·œì¹™.md í•­ëª©ì„ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ê³  ê° í•­ëª©ì— ëŒ€í•´ ì¤€ìˆ˜/ë¯¸í¡ + í•œ ì¤„ ê·¼ê±°
""".strip()


def _split_critique_chunks(text: str, max_chars: int, max_parts: int) -> list[str]:
    normalized = (text or "").strip()
    if not normalized:
        return [""]
    paragraphs = re.split(r"\n\s*\n", normalized)
    chunks: list[str] = []
    current: list[str] = []
    current_len = 0

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        if len(para) > max_chars:
            if current:
                chunks.append("\n\n".join(current))
                if len(chunks) >= max_parts:
                    return chunks
                current = []
                current_len = 0
            start = 0
            while start < len(para):
                part = para[start:start + max_chars]
                chunks.append(part)
                if len(chunks) >= max_parts:
                    return chunks
                start += max_chars
            continue

        extra_len = len(para) + (2 if current else 0)
        if current_len + extra_len > max_chars:
            chunks.append("\n\n".join(current))
            if len(chunks) >= max_parts:
                return chunks
            current = [para]
            current_len = len(para)
        else:
            current.append(para)
            current_len += extra_len

    if current and len(chunks) < max_parts:
        chunks.append("\n\n".join(current))

    return chunks


def _extract_chunk_summary(text: str) -> str | None:
    if not text:
        return None
    match = re.search(r"^\(0\)\s*íŒŒíŠ¸ ìš”ì•½[:ï¼š]?\s*(.+)$", text, re.M)
    if match:
        return match.group(1).strip()
    match = re.search(r"^íŒŒíŠ¸ ìš”ì•½[:ï¼š]?\s*(.+)$", text, re.M)
    if match:
        return match.group(1).strip()
    return None


def _fallback_chunk_summary(content: str, max_chars: int = 300) -> str:
    trimmed = re.sub(r"\s+", " ", (content or "").strip())
    return trimmed[:max_chars] if trimmed else "ìš”ì•½ ì—†ìŒ"


def _assemble_chunked_critique(chunk_outputs: list[str], overall_text: str) -> str:
    parts: list[str] = ["## íŒŒíŠ¸ë³„ í•©í‰"]
    total_parts = len(chunk_outputs)
    for idx, text in enumerate(chunk_outputs, start=1):
        body = (text or "").strip()
        parts.append(f"### íŒŒíŠ¸ {idx}/{total_parts}\n{body}")
    parts.append("---\n\n## ì „ì²´ í•©í‰\n" + (overall_text or "").strip())
    return "\n\n".join(parts).strip()


@app.post("/api/critique", operation_id="api_critique")
async def api_critique(req: CritiqueRequest):
    opts = req.options or CritiqueOptions()
    # ì…ë ¥ ì›ê³  ì €ì¥ (í•­ìƒ ìˆ˜í–‰)
    work_path = _save_critique_object(req.title, req.content)

    # ê¸°ì¤€ ë¡œë“œ ë° LLM í•©í‰ ìƒì„±
    criteria = _load_critique_criteria()
    critique_text: str
    if opts.chunked_critique:
        max_chars = opts.chunk_max_chars or CRITIQUE_CHUNK_MAX_CHARS
        max_parts = opts.max_parts or CRITIQUE_CHUNK_MAX_PARTS
        chunks = _split_critique_chunks(req.content, max_chars=max_chars, max_parts=max_parts)
        chunk_outputs: list[str] = []
        part_summaries: list[str] = []
        total_parts = len(chunks)

        for idx, chunk in enumerate(chunks, start=1):
            chunk_prompt = _build_critique_chunk_prompt(
                req.title,
                chunk,
                criteria,
                part_index=idx,
                total_parts=total_parts,
                extra_instruction=req.extra_instruction,
            )
            chunk_text = await call_llm(chunk_prompt)
            chunk_outputs.append(chunk_text)
            summary = _extract_chunk_summary(chunk_text) or _fallback_chunk_summary(chunk)
            part_summaries.append(summary)

        overall_prompt = _build_critique_overall_prompt(
            req.title,
            part_summaries,
            criteria,
            extra_instruction=req.extra_instruction,
        )
        overall_text = await call_llm(overall_prompt)
        critique_text = _assemble_chunked_critique(chunk_outputs, overall_text)
    else:
        prompt = _build_critique_prompt(req.title, req.content, criteria, extra_instruction=req.extra_instruction)
        critique_text = await call_llm(prompt)

    critique_path: str | None = None

    # âœ… responseì— ë°˜í™˜í•  critique md(í—¤ë” í¬í•¨)ë¥¼ ë§Œë“ ë‹¤
    now_iso = datetime.now().isoformat()
    critique_yaml_header = {
        "type": "critique",
        "object_title": req.title,
        "created_at": now_iso,
        "updated_at": now_iso,
        "source_object_file": work_path,
        "usage": ["critique"],
    }
    critique_md_with_header = "---\n" + yaml.safe_dump(
        critique_yaml_header, allow_unicode=True, sort_keys=False
    ) + "---\n\n" + critique_text

    # âœ… ê²€ì¦ (front-matter + ì£¼ìš” í•„ë“œ)
    meta, _, has_fm = extract_front_matter(critique_md_with_header)
    if not has_fm:
        raise ValueError("critique output missing front-matter (server bug)")
    validate_critique_front_matter(meta)

    if opts.save_critique or (opts.save_work is True):
        # ê¸°ì¡´ ì €ì¥ ë¡œì§ ìœ ì§€ (ì €ì¥ íŒŒì¼ë„ ë™ì¼í•œ í—¤ë” í¬í•¨)
        critique_path = _save_critique_result(req.title, critique_text, work_path)

    return standard_response(
        success=True,
        message="ok",
        data={
            "critique": critique_md_with_header,  # âœ… ì´ì œ í—¤ë” í¬í•¨ mdë¡œ ë°˜í™˜
            "critique_file_path": critique_path,
            "work_file_path": work_path,
        },
        error=None,
    )


@app.get("/health")
async def health():
    return {"status": "ok", "time": datetime.now().isoformat()}
